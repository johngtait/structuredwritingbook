<?xml version="1.0"?>
<db:chapter xmlns:db="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:strings="http://exslt.org/strings" version="5.0" xml:id="chapter.metadata">
<db:title>Metadata</db:title>

<db:para>We live in the age of metadata, so much so that the word metadata has almost come to replace the word data itself and has come to be applied to almost any form of data that describes a resource. For example, we hear a lot about law enforcement getting access to metadata related to phone calls, which simply means the data about which number called which number and for how long.</db:para>
<db:para>In the <db:xref linkend="chapter.definition"/> I said that structured writing is writing that obeys constraints and that records the constraints it obeys. The standard definition of metadata is data that describes data. This can make it seem like an optional extra, as something added on top of something that already exists. But actually there is no data without metadata. Data is a formalization of information. It constrains the interpretation of the information. Metadata is the record of those promises/constraints. Without it, you have no formal assurance that the information is what it seems to be.</db:para>
<db:para>Structure is the imposition and annotation of constraints. The recording of constraints is metadata. Structure and its annotations, therefore, consist of metadata. Where you have structure, you have metadata. Where you have metadata, you have structure.</db:para>
<db:para>Recipes contain lists of ingredients. If you record a recipe in the document domain, you will record that list of ingredients using a list structure. In this case the constraint you are imposing, and therefore the metadata you are recoding is: this is a list and these are list items. This metadata does not record that the items in the list are ingredients. Human beings recognize the list of ingredients for what it is based on familiarity with the pattern and recognition of the names of foodstuffs. This is fine for humans, but still a little iffy for our current generation of AI algorithms. Writing an algorithm to reliably recognize ingredients in recipe, while not impossible, is a lot of work.</db:para>
<db:para>Move that recipe into the subject domain, however, and you replace a list structure with an ingredients structure. In this case the constraint you are imposing, and therefore the metadata you are recording, is: this a a collection of ingredients, and each one of them is an ingredient. Writing a reliable algorithm that recognizes ingredients based on explicit metadata that says they are ingredients is trivial.</db:para>
<db:para>This is not to say that a list structure is not metadata also. It is. It is document domain metadata. It formally identifies the list items structure as a list item. It makes it easy to write algorithms that recognize lists. This  allows you to reliably format it as a list item in whatever media you choose to publish it in. But subject domain metadata that identifies an ingredient as an ingredient provide a different promise, one that allows us to do more with the data.</db:para>
<db:section>
<db:title>The recursive nature of metadata</db:title>
<db:para>Metadata is confusing because it is recursive. If metadata is the data that describes data, it is also the data that describes the metadata. The data/metadata distinction is not one of type -- metadata is not a different kind of thing from data -- it is one of relationship. It is a father/son distinction, not a cat/dog distinction. One man can be both a father and son. What we call him depends on what relationship we are looking at at a given moment.</db:para>
<db:para>In structured writing, we add structure to content to replace the things we have factored out. That structure is metadata to the data that is the text of the file. But if we store that file in any kind of repository, the information that identifies the file in that repository is metadata to the file as a whole. But that is not the only kind of metadata for that file. If the structure of the file is described by a schema, the schema is also metadata for the file.</db:para>
<db:para>But we’re not done yet because the specification of the schema language is the metadata that tells you what the schema means.  And then of course, there is the specification of the markup to consider. The XML specification is part of the metadata for every XML document in existence. And we are still not be done, because the XML specification uses a formal grammar description language, called BNF. The BNF specification is metadata for the schema language description.</db:para>
<db:para>So, every piece of data has a spreading tree of metadata supporting it, which, if traced to its roots, eventually leads to plain language documents that explain things in human terms.</db:para>
</db:section>
<db:section>
<db:title>Where should metadata live?</db:title>
<db:para>One of the great questions about metadata is where it should live: with the data it describes or separate from it? For much metadata the answer is obvious. It lives separately because its scope is wider than one resource. An XML document does not include the XML spec, for instance. But for metadata that is unique to an individual resource, the question is an important one.</db:para>
<db:para>Most early graphic file formats only stored the image. Most modern format also store extensive metadata about the image. The pictures you take with your digital camera include lots of information about the camera and the settings that were used to take the shot, and even the geographic coordinates of where the shot was taken, if the camera has a GPS receiver. Having that metadata embedded in the file ensure that the picture and its metadata stay together.</db:para>
<db:para>In part the argument is about who is responsible for creating the metadata. In the case of the photo, the metadata is in the file because the camera is the best placed instrument to record it. Sometimes it is about whether the metadata is intrinsic or extrinsic to the content. Sometimes it is about control.</db:para>
<db:para>For example, should the history of a file be stored in the file or in the repository? Storing it in the file lessens the file’s dependence on the repository and makes it more portable. But a repository vendor may prefer to sell you a system in which to uninstall their repository would be to loose all your file history. If file status information is only stored in a workflow system, for instance, it is very hard to move away from that system. It it is stored in the file, it is easy to move away, and also to edit when not connected to the system, which can save you on licenses.</db:para>
<db:para>Writing you content in the subject domain means that more of your metadata is stored in the same file as the content, increasing its independence and portability. Also, as we have seen, the use of subject domain structures can lessen the need for management domain structures for algorithms like single sourcing and content reuse, which reduces the need for external management domain metadata.</db:para>
</db:section>
<db:section>
<db:title>Metadata enables algorithms</db:title>
<db:para>The purpose of attaching metadata to content is to make the content into data. The reason for making content into data is to make it accessible to algorithms. The best way to think about metadata is in terms of the algorithms it enables.</db:para>
<db:para>Basic publishing algorithms can be performed using document domain metadata alone. But as we have seen, publishing-related algorithms such as differential single sourcing, content reuse, and content generation also require subject domain metadata. Establishing the relevance of content requires subject domain metadata because we search for content based largely on its subject matter. Validating, which deals with whether content meets its defined constraints, depends on the metadata of the domain in which the content is recorded. Auditing, which deals with whether content meets business requirements, generally requires subject-domain metadata as most business requirements are in the subject domain.</db:para>
<db:para>But this does not mean that all content gets captured in the subject domain. There is another way to apply subject domain metadata to content besides recording the content in the subject domain. This is to apply subject domain metadata externally to the content or an an overlay to the document domain structures in which the content is recorded.</db:para>
<db:para>For example, if you wanted to have access to the data on which ingredients were mentioned in each recipe (perhaps as an aid to finding recipes), you could record the recipe itself in the document domain and then place it in a relational database table with a many-to-many relationship to a table that listed all know ingredients.</db:para>
<!--  Example -->
<db:para>Systems designed primarily for document domain content creation, such as DITA, can include formalized structures for attaching subject domain metadata to document domain structures. In DITA, the primary such mechanism is the subject schema.</db:para>
<db:para>One of the primary contrasts between the document domain and the subject domain then is not that subject domain metadata is entirely missing from the document domain, but that in document domain systems, subject domain metadata is stored separately from the document domain markup, whereas in the subject domain it is integrated into the content itself.</db:para>
<db:para>But the two approaches are not equivalent. They vary substantially in capability, implementation requirements and ease of use.</db:para>
<db:para>For example, when we talked about differential single sourcing we saw that subject domain content could be transformed into radically different document domain structures for publication in different media. (A table for paper vs. an interactive widget online, for instance.) Attaching subject domain metadata to a piece of content that contains a document domain structure (such as a table) does not give you the capability to differentially single source that table to different structures for other media.</db:para>
<db:para>One area in which external subject-domain metadata excels is finding and filtering. You can certainly find and filter on subject domain content, but because there are many different subject domain structures, it can be non-trivial to figure out how to write the query expression that does the finding and filtering you require. By contrast, external subject domain metadata can be stored in normalized data structures that are consistent and therefore easier to write queries for. This consistency also allows tools and systems to put high-level interfaces over these query mechanisms for users that don’t know how to write queries themselves. As such, the approach is widely favored in content management systems.</db:para>
<db:para>On the other hand, there is a limit to how fine grained a find and filter operation can be with external metadata. If you want the advantage of external metadata stored in normalized structures for ease of querying, it can only discriminate down to the level of the pieces of content it is applied to. It can retrieve that whole content unit, but not pieces of it.</db:para>
<db:para>For validation, it comes down to a matter of what constraints you want to validate against. As we have seen, moving content to the subject domain allows us to factor out many of the constraints that we want to apply to the document domain and to express and enforce the constraints we want to apply to how the content treats its subject matter. If content quality is your primary reason for adopting structured writing, these advantages are hard to ignore.</db:para>
<db:para>For auditing, the situation is much the same. If you are looking to audit the consistency of your content, document domain metadata will not suffice, as we noted in <db:xref linkend="chapter.conformance"/>. If you are looking to audit your coverage of subject matter, external subject domain metadata attached to document domain content will give you a high level look at your coverage, but tell you little about quality or how well subjects are being covered.</db:para>
<db:para>When it comes to ease of use, there are things in favor of each approach. If you have standardized document structures and standardized metadata structures, then users only have to learn those structures.</db:para>
<db:para>On the other hand, separating subject metadata from the writing of the content itself means that writers have to do two separate jobs. Rather than creating the metadata as they write in the subject domain, they have to write in the document domain and then create separate subject domain metadata to describe the document they have just written. This creates more work for the writer, and has frequently proven error prone.</db:para>
<db:para>This can significantly impact the quality of both the content and the metadata. Because the document domain does not impose any constraints on how a subject is treated, it it quite easy to apply subject domain metadata to content that simply does not merit it. Since metadata is supposed to express a promise that certain constraints have been met, this creates a broken promise.</db:para>
<db:para>The fact is, promising that a constraint has been met without validating that it has actually been met is a recipe for broken promises. Attaching subject domain metadata externally to document domain structures always runs this risk. Subject-related constraints have not been factored out or enforced. There has not even been effective subject-domain guidance provided to the writer while writing the content.</db:para>
<db:para>I noted above that external subject domain metadata excels in the area of finding because it makes it easy to write queries on standardized metadata structures. But there is a large caveat to this, which is data quality. If the promises the metadata makes are not kept by the content itself, the value and reliability of searches is severely diminished.</db:para>
<db:para>At the end of the day, the value of any system is limited by the quality of its data. Content management systems often fail or underperform because of poor quality data caused by a poor fit between content and its metadata. In particular, there is a tendency for data quality to decline over time. Systems based on external subject-domain metadata need to be aware of the problem and vigilant to prevent it.</db:para>
<db:para>We should also note that the two approaches are not mutually exclusive. The subject metadata used in both cases is essentially the same, since it describes the same subject. There is no reason, therefore, why you cannot take subject domain content, with its integrated metadata, and extract the metadata to an external metadata store. This way you can use the same query interface to find the content without giving up the validation and auditing capability of subject domain content or the finer-grained application of metadata that the subject domain enables. This approach can significantly improve the quality of the metadata in the system because all the subject domain guidance and constraints have been applied to the content before it is stored. Thus, as long as the content itself is correct, there cannot be a mismatch between what the metadata promises and what the content delivers.</db:para>
<db:para>There are ease-of-use advantages to his approach as well. Since the CMS metadata is extracted from the content metadata, the writer does not have to enter the metadata separately from the content after is is written. If your take such an approach, however, it is vital to be very clear about which is the canonical source of the metadata. The subject-domain content is the canonical source and the only one that should ever be edited. The exported metadata in the CMS is a cache of the content metadata created for the sake of optimizing searches. Of course, if your CMS was based on an XML database, you would not need to cache the metadata in relational tables; you could query it directly.</db:para>
</db:section>
<db:section>
<db:title>Taxonomy and controlled vocabulary</db:title>
<db:para>With subject domain metadata, whether is is stored internally in the content or externally in a CMS, it is important to be consistent in how subjects are named. Of course, it is also important in the content itself that key subjects be named consistently across the content set to avoid confusion for readers. If you are supplying a large-scale top-down navigation scheme for your content, that scheme is also going to be concerned with the names of things, and with choosing the right terms to name the subjects so that readers can find them.</db:para>
<db:para>For all of these reasons, large scale content projects need to exercise some control over terminology. (Control of terminology is important for translation as well, but that is outside the scope of this book.) There are two parts to the terminology control problem: establishing terminology, and enforcing its use.</db:para>
<db:para>The principal difficulties in establishing terminology are:</db:para>
<db:itemizedlist>
<db:listitem>
<db:para>Human being have a fairly small “use vocabulary” and we reuse words all the time. In safety critical functions like air traffic control or the operating room, we train everyone to use a special unambiguous vocabulary (which is generally undecipherable to the layperson) but for most uses, the words we want to control are likely to be used in multiple ways that are not easy to disambiguate formally.</db:para>
</db:listitem>
<db:listitem>
<db:para>People in different fields (even within the same organization) often use different terms for the same concept. What the chef calls “pork”, the farmer calls “pig”. What the English call “boot”, North Americans call “trunk”. Trying to force everyone to use the same term means forcing them to say things that don’t make sense in their own field.</db:para>
</db:listitem>
<db:listitem>
<db:para>People in different fields (even within the same organization) often use the same words to mean different things. What the conference organizer calls a function is not what the programmer calls a function. What a programmer calls a function is (in more subtle ways) not what a mathematician calls a function.</db:para>
</db:listitem>
<db:listitem>
<db:para>People in one field may have ten terms that make fine-grained distinctions among things that people in another field lump together under a single term. For instance, programmers make a distinction between subroutines, functions, methods, and procedures. I know of one documentation project on which it was mandated that all of these should be called “routines”. This was OK most of the time, but because a problem when function pointers were introduced into the product. (You can’t use any of the other terms with “pointer”.) Enforcing a single term obscured a distinction the turned out to matter.</db:para>
</db:listitem>
</db:itemizedlist>
<db:para>In short, very little of our terminology is truly universal. The meaning of words changes depending on the context you use them in and the audience you are addressing. This is, in fact, at the core of how content works and why it is different from other forms of data. Content tells stories, and what words mean depends on the context in which they are used in the story. Stories are not as precise as formal data, but the only way we have to define formal data is with stories. This is why the spreading tree of metadata that supports and explains any point of data always ends with a human-language document. All of structured writing is an attempt to bring some small part of the orderliness and manageability of data to stories to improve the quality and consistency of stories so that we can communicate more effectively.</db:para>
<db:para>Thus terminology control is important, because it is key to making our structures and metadata work, but at the same time it must be done with care and sensitivity and a real sense of the limits within which it is possible to control the terms we use to tell stories. Good taxonomies always confine themselves to specific domain and define their terminology within the confines of those domains, but even within all but the most strictly controlled domain (such as the operating theater or the control tower) there are still commonly many shades of meaning on individual words which are only fully disambiguated by the story.</db:para>
</db:section>
<db:section>
<db:title>Top-down vs. bottom-up terminology control</db:title>
<db:para>You can define terminology top-down or bottom up. The principle tools of top-down definition are controlled vocabularies and taxonomies. A controlled vocabulary is essentially a list of terms and their proper usage within a specific domain.</db:para>
<db:para>A taxonomy is a more elaborate scheme for controlling and categorizing the name of things. Taxonomies are frequently hierarchical in nature, defining not only the terms for individual things but the names for the classes of things. Thus a taxonomy does not just list sparrows and blue jays and robins, it also classifies them as birds, and birds as animals, and animals as living things. A good taxonomy should be specific to the domain for which it is intended. (Blue Jays and Cardinals occupy a very different place in a baseball taxonomy than in a ornithological taxonomy.) As a classification scheme, a taxonomy may be used not only as the basis for controlling vocabulary, but also as a basis for top-down navigation of a content set.</db:para>
<db:para>Alternatively, you can control terminology from the bottom up. To do this, you use subject domain annotations in your content to highlight key terms and place the usage in the appropriate domain.</db:para>
<db:programlisting language="sam">
In {Rio Bravo}(movie), {the Duke}(actor "John Wayne") plays 
an ex-Union colonel.
</db:programlisting>
<db:para>In the passage above, the annotations call out the fact that “Rio Bravo” is the name of a movie and that “the Duke” is the name of an actor called “John Wayne”.</db:para>
<db:para>This is taxonomic information. It places “Rio Bravo” in the class “movie” and “John Wayne” in the class “actor”, with the added information that “the Duke” is an alternate term for the actor John Wayne. By placing these terms in these classes, it makes it clear that “Rio Bravo” in this context is not the Mexican name for what Americans call the Rio Grande or any of the several American towns named Rio Bravo, and that “the Duke” refers to John Wayne and not to The Duke of Wellington (who was often called by that nickname) or any of the possible meanings of “Duke”.</db:para>
<db:para>By querying a content set that contains such annotation (and possible querying other subject domain information it contains as well) it is possible to extract a taxonomy from your content set.</db:para>
<db:para>To appreciate why this might be useful, we can look at some of the difficulties of maintaining and enforcing  vocabulary constraints.</db:para>
<db:para>As mentioned above, constraining vocabulary is hard because the same term can mean different things in different contexts, and different terms can mean the same thing in different contexts. If you attempt to build a taxonomy from the top down it can be very difficult to anticipate the various meanings a term may have in different contexts. Even if you study existing texts, there is no guarantee that you will exhaust all the possibilities, and such searches are tedious and time consuming.</db:para>
<db:para>Secondly, even once you have defined your taxonomy from the top down, there is the question of how it is going to be enforced. You can require authors to use terms from the taxonomy, but how is that going to work? Do you expect them to carry the entire taxonomy around in their heads? And as they are writing, do you expect that they will recognize that they are using a word not on the taxonomy every time they refer to a subject covered by the taxonomy? Any attempt to comply with such requirements is going to create a lot of mental overhead for writers, and, as we have noted (<db:xref linkend="chapter.authoring"/>) dividing author’s attention has a negative impact on content quality.</db:para>
<db:para>There are mechanical solutions that attempt to catch vocabulary problems, but the fact that words can mean so many things in different contexts means no such process can get it right all the time.</db:para>
<db:para>An alternative to first trying to define and then trying to an enforce a taxonomy from the top down it to let it emerge, in a disciplined way, from the content itself. The key to this is that in structured writing, particularly in the subject domain, you annotate those things that are significant to your content. If the vocabulary you are trying to enforce is not vocabulary that is significant to your content, you are probably wasting your time, so the vocabulary you want to use and the subjects you want to annotate should overlap.</db:para>
<db:para>If you have writers annotate the significant subjects in their content as they write, they will be annotation those very terms whose vocabulary you want to control. So when they mention the name of a bird like Blue Jay or Robin, they annotate it as <db:code>{blue jay}(bird)</db:code> or <db:code>{robin}(bird)</db:code>.</db:para>
<db:para>Now instead of building your list of birds from the top down, you can simply have an algorithm scan your content an create a list of all the birds named in the content. By scanning that list, you will be able to tell if the names of birds are being used inconsistently.</db:para>
<db:para>If an author mentions a new bird that you would not have thought to include in your taxonomy, it will get added to the list. You can have an algorithm alert you ever time a new bird is mentioned by an author. Effectively, now, your taxonomy is bubbling up from your content. Your authors are not having to worry about whether the terms they are using are in the taxonomy or not, as long as they mark up what type of thing they are naming. But your will still get an alert every time a new term is added, and will be able to evaluate if it is being used correctly.</db:para>
<db:para>It is possible that some authors will forget to annotate some birds, or will annotate them incorrectly (as something other than a bird). But we can easily catch most of these mistakes as well. Incorrect annotations will tend to show up as anomalous entries in other annotation categories. In the cases where there are genuine name conflicts between two different domains, you can make a list of such conflicting names and use and algorithm to compile a list of all tagged instances of those names to review for incorrect tagging. For failure to tag at all, you can use the generated list of tagged terms to compile a list of terms to check for and periodically scan the content set to pick up significant cases.</db:para>
<db:para>Another useful conformance tool that can come out of the bottom-up approach is a stop list. A stop list is a list of terms that should not be used, but frequently are. It can be used by an algorithm to scan content for inappropriate vocabulary. Stop lists can only really be created bottom up. You can’t anticipate or ban ever term anyone might ever come up with. You should only ban terms that are both problematic and which occur frequently. (The chance of false hits -- of banning terms that are perfectly legitimate in context -- rises with every word you add to the stop list.) With a bottom up approach to terminology control, you get an accurate measure of which terms are being misused, and the frequency and nature of the misuse. This is an excellent basis for compiling a useful stop-list.</db:para>
<db:para>Also, because subject annotation can specify the type of a term (that is distinguish between <db:code>{Blue Jays}(baseball-team)</db:code> and <db:code>{Blue Jays}(bird)</db:code> you can make your stop list type specific: banning works used in one sense but not another. This can greatly reduce false hits, which is important because people tend to abandon the use of conformance tools if they produce very many false hits, as we can see from the very infrequent use of the grammar checkers in Word Processors.</db:para>
<db:para>As your taxonomy develop from the bottom up, it may be tempting to start to impose it from the top down, but this is probably not a good idea (though there may be exceptional circumstances). Having to comply with an top-down taxonomy while writing divides the writer’s attention and compromises functional lucidity, both of which are bad for content quality and the author’s efficiency. Continuing to rely on feedback to catch errors after initial authoring allows writers to stay in the moment as they write. If the feedback is given to that authors and they fix the errors themselves, they will learn the correct terminology and tagging over time and will make fewer mistakes. However, the door will remain open for new terms to be allowed into the content as appropriate, and to bubble up to the taxonomy, rather than banning them from the content until the taxonomy can be updated from the top down.</db:para>
<db:para>This approach does not make all vocabulary constraint problems go away, but it does have a number of advantages.</db:para>
<db:itemizedlist>
<db:listitem>
<db:para>It turns an up-front taxonomy development effort into an permanent part of the content development process. This not only reduces the spike in effort, it means that the taxonomy is based on real experience writing real content and that it is continually maintained as subjects and business objectives change. Indeed, if you are already annotating your content  to support any of the other structured writing algorithms, you essentially get taxonomy development, maintenance, and control almost for free. (Someone does have to review the reports, make the edits, and add to the canonical taxonomy, of course.)</db:para>
</db:listitem>
<db:listitem>
<db:para>It improves functional lucidity by not forcing authors to refer to the taxonomy while writing. If you are already annotating for other reasons, you are imposing no additional burden on authors at all.</db:para>
</db:listitem>
<db:listitem>
<db:para>By improving the consistency of the annotations, it makes all the other algorithms that rely on them more reliable as well. (This is one of the greatest virtues of the subject domain. Subject domain markup can serve multiple algorithms, meaning you get the benefits of multiple algorithms with less cost.</db:para>
</db:listitem>
</db:itemizedlist>
</db:section>
<db:section>
<db:title>Ontology</db:title>
<db:para>Finally it is worth saying a word about ontology. Ontology (in the information processing sense) is an attempt to create a formal mapping of the relationships between entities in the real world such that algorithms can draw inferences and reach conclusions about them.</db:para>
<db:para>In many way, therefore, an ontology is an attempt to do for algorithms what content does for humans. After all, one of the main reasons that we read is so that we can understand the world better, understand what various objects and institutions are and how they relate to each other, statically and in action, so that we can decide what to do.</db:para>
<db:para>In some sense, therefore, ontology is the ultimate in subject domain markup. And just as subject domain markup can be used to drive content management functions, so there are attempts to use ontologies to drive content management functions, especially at the enterprise level. One should also be able to generate human-readable content from an ontology, given a sufficiently sophisticated algorithm and a sufficiently sophisticated ontology.</db:para>
<db:para>All of this is very much outside our scope in this book. Subject domain markup is an attempt to capture certain aspects of the subject matter of a work. But it is not an attempt to model the argument of a work. Given the passage:</db:para>
<db:programlisting language="sam">
In {Rio Bravo}(movie), {the Duke}(actor "John Wayne") 
plays an ex-Union colonel.
</db:programlisting>
<db:para>Here the subject domain markup is content to formalize the fact that Rio Bravo is a movie and that “the Duke” is a reference to the actor John Wayne. It does not model the relationship between the two. An ontology would want to model the “starred in” relationship between John Wayne and Rio Bravo, whereas the subject domain is content to leave this to the text.</db:para>
<db:para>Similarly, this subject domain markup does not bother to denote that Union is a reference to both a country and its armed forces, and that colonel is a rank in those armed forces. It does not denote these things because this particular markup language is concerned with movies and these facts are entirely incidental to the movie business. A full ontological treatment of the passage above, however, would need to model those relationships.</db:para>
<db:para>Structured writing does make certain aspects of content clear to algorithms, but not with the intention of making it possible for the algorithms to make real-world inferences and decisions based on the information in that content. It only does what is necessary to allow human authors to use algorithms as tools to improve the quality of the content they prepare for human readers.</db:para>
</db:section>
</db:chapter>
