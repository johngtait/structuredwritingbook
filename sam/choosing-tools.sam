chapter:(#chapter.tools) Choosing Tools

    The standard advice on setting up a structure writing system is that you should choose your tools last. That is, that you should decide on you business goals first, then choose the structured writing domain and/or specific language you want to use, and then, and only then, go shopping for tools that meet those requirement. 
    
    This sounds good in theory, but it is not quite so simple in practice. Different types have different costs and require different skill sets. To entirely ignore tools until everything else is decided could lead you to choosing a system that is not economical or is too hard for you to use or maintain. And if {functional lucidity} is one of your goals, it cannot be separated from tools. 
    
    Here is another way to think about it. Tools are not just hardware and software. A structured writing tagging language is a tool. It is a tool you will select or design and it is the tool with which your writers will interact the most in their day to day work. The tagging language you choose or create will have a far greater impact on {functional lucidity}, and on your ability to use the structured writing algorithms, than any choice you make about editors or repositories. 
    
    But that does not mean that the rest of the tool chain should not figure into your decision as well. Different languages, different algorithms, and different approaches to implementing different algorithms differ widely in the cost of tools to support them, and in the skills needed to create, maintain, and use those tools. Committing to any language, or any set of algorithms, without considering the cost of implementing and maintaining the entire tool chain would be short sighted. 
    
    For example, {MarkDown} is a very basic tagging language. It gives you basic {separation of content and formatting} and a high degree of {functional lucidity} for simple documents. It basic tools are free to download. You can use it in concert with simple content management systems that are free and easy to use, and with a {static site generator} like {Jekyll} and a {template system} like {Liquid}, you can add support for {reuse} and {single sourcing} at the expense of leaning some mildly exotic {management domain} markup and some system configuration tasks.[*1]

        footnote:(*1)
            Tom Johnson has a good explanation of how a system based on MarkDown and Jekyll can work at {http://idratherbewriting.com/2015/03/23/new-series-jekyll-versus-dita/}(url).

    This a approach does not get you meaningful constraints, conformance testing, sophisticated management of reuse, change management, or many of the other content management algorithms. It does very little to improve content quality other than freeing authors to focus on content through it high degree of {functional lucidity}, and it is no way to write a book. But it is much lower cost then perhaps any system that would let you do those things. Doing something more than this, therefore, demands a business case that anticipates greater savings and or revenue benefits (from improved quality or greater monetization of content assets) through investment in a more complex system. Those benefits are certainly available, but you should have a clear idea of where they are going to come from, as well as the real costs of a more complex system.
    
    section: Monoliths vs tool chains
    
        If you have worked with {word processors} and {desktop publishing} applications you are used to a monolithic applications that does writing, document design, and publishing in one application through one interface. Monolithic tools of this kind are actually something of an anomaly. Before computers, the publishing process was a tool chain. Writers wrote with a pen and later a typewriter. Typescripts were then marked up by editors and document designers and handed off to typesetters who prepared the type for printing.
        
        In the computer world too, such applications are something of an anomaly. The large monolithic application that did everything in one interface was a product of the growth of powerful personal computers that largely lacked network connections before the Internet became affordable for most people. Computing actually works more naturally in a network environment where functionality can be distributed between many processes and data can be passed as messages between processes. The Web has brought this mode of operation to the fore again, particularity in mobile, where small aps working in concert with network services are the norm rather than monolithic applications -- whose interfaces would not fit on most mobile screens.  

        In the world of structured writing the use of tool chains -- multiple independent programs communicating with each other via messages -- is more the norm. This makes sense when we consider that the point of structured writing is to enable an number of different algorithms to operate on content, and that in many cases those algorithms need to be custom written to process custom content structures. Each organization needs a different set of pieces and the best way to put the right pieces together is to use create independent applications that can be easily integrated to create a tool chain. 

        Even if you buy what is nominally a turnkey system, such as a DITA CMS, it is really a tool chain. Most CMS's, for instance, will use a third-party XML editor such as {ArbourText} or {Oxygen}, a third party formatter such as {Antenna House} or {Apache FOP}, a third party XSLT engine, a third party database, and the open source DITA Open Toolkit. Buying a tool chain that someone has assembled and integrated for you has obvious benefits over assembling and integrating one yourself (supposing that you get the same functionality) but even with the turnkey systems you should still pay attention to how open the system is to your plugging additional elements into the tool chain.         
    
    section: Editors 
    
        If you are using an XML-based {markup language}, you are almost certainly going to need an XML editor for each of your authors. The {functional lucidity} of raw XML is poor, so an editor that can show a graphical view of the document is almost a necessity.
    
        It is not just the ability to hide the verbosity of XML that matters in an editor, however. Depending on the structured writing algorithms you are using, and the way they are implemented by your chosen markup language, you may require help from your editor. For instance, if you are doing ah-hoc  {content reuse} you may need a way to see the results of that reuse in order to judge if the material is being reused correctly. If you are doing variable-into-common or common-into-variable, you may need a way to see the imported material live in the editor, and then to switch between different build scenarios in order to see that the result of the reuse in in each case. If you are doing complex conditional text, you may also want an editor that lets you apply the various conditions interactively to check the results. 
        
        Needless to say, having to do these kinds of checks compromises the {functional lucidity} of your system, but it compromises it far more if you don't have a way to check what you are doing. Avoiding this type of reuse in favor of a more structured approach may be a better choice, but if not, good editor support for previewing the results is crucial.
        
        XML's verbose and unreadable syntax means that the use of an XML editor with a graphical view is going to be a requirement for most authors. As we have noted, this is something of a catch-22, since the use of a graphical view makes it harder for the author to see the structure they are supposed to be creating and can encourage them to think more in {media domain} terms rather than {document domain} terms. 
        
        Lightweight markup languages, of course, are designed to be usable without a specialized editor. This has the twofold advantage or improving {functional lucidity} by allowing the author to see the structure without it obscuring the text, and avoiding the cost of a sophisticated XML editor. There are editors for some lightweight languages but they tend not to hide the markup. Rather, they provide syntax highlighting and/or a preview panel so that the author can see the formatting of the document as they type the markup. 
        
        Of course, this means that if you want the support of the editor for things like previewing reuse, you are not likely to find support for that in lightweight languages because you are unlikely to find an editor for them with those kinds a capabilities. On the other hand, part of the problem with reuse structures in XML-based languages is that the markup itself gets hidden in the editor's graphical view, so without the editor pulling in the reused content dynamically, the reuse instruction would be invisible by default (though it could be made visible with the right code). But in a lightweight language, the reuse structure would be visible to the author. 
        
        That leaves us with the question of whether {functional lucidity} is better served by seeing the result of the reuse or by seeing the logic of reuse (in the form of the structure that invokes it). It also helps demonstrate how much the issues of structure, lucidity, and tools are intertwined.
        
        Another issue to be aware of is that some editors can add functionality not found in the markup language itself. For instance, some XML editors will add support for the kind of revision tracking you are used to in Word Processors like {Microsoft Word}. They do this by adding additional information to the XML file, usually using an XML feature called processing instructions. Here is an example from the popular XML editor, {Oxygen}(tool "Oxygen XML Editor):
        
        ```(xml)
            <db:chapter>
                <db:title>Buy vs. Build</db:title>
                <db:para> Not <?oxy_delete author="Mark" timestamp="20160911T142133-0400" content="available"?> yet.
                    <?oxy_insert_start author="Mark" timestamp="20160911T142048-0400"?>But soon.<?oxy_insert_end?>
                </db:para>
            </db:chapter>
            
        Other processes would ignore the processing instructions in this, and so they would just see the revised text. But Oxygen can show both the original and the revision. 
        
        Again, this is functionality you are much less likely to see with lightweight markup languages. On the other hand, if you store your content in a version control system such as SVN or Git, you can use a utility call Dif (for Difference) to show you the difference between any two versions of a file without every having to turn on change tracking and without any additional markup being inserted into the file. The version control system can also show you exactly who made each change in a file when. Overall it is a much more complete and sophisticated system than word-processor-style change tracking, but it is a very different paradigm that many authors are not used to. 
        
    section: CMS vs VCS
    
        At the time I write there is a growing movement to "treat docs like code". This means writing content in plain text (which generally means in a lightweight markup language), to publish it using a build process similar to that used for software, and to store the source files in a version control system similar to those used by software developers (and in many organizations that means in the same repository used for the code of the product being documented).
        
        This approach is in marked contrast to the content management system model which has dominated large scale publishing, as well as traditional web publishing, for a long time. Content management systems have traditionally tried to take a comprehensive approach to the writing a publishing process. Everything works through the CMS, often through a single desktop interface. Many XML editors, for instance, are designed to integrate with major content management systems so that they become the front end for those systems. CMS are often workflow-oriented, concerned with moving content through a process. They generally have their own repository to which they manage access. Most have little direct support for working outside the system or bringing content into the system dynamically. (They will allow you to bring content in statically as a one time conversion on the assumption that you will work on it in the system from now on.) Most, therefore have little or no support for integrating content from multiple sources or processes. 

        A version control system like {Git} or {SVN} on the other hand, is just a file-system based repository with support for version control and branch and merge operations. Branching is a process by which you create a copy of a content set to work on. You work on your branch and other writers work on their own branches. When your work is ready, you merge it with the main branch, which means that your changes are integrated back into the version that will be published. 

        Branching and merging means that the main branch of the content can be kept in a state where it is always ready to publish. When a writer finishes some changes and wants to publish them, they merge their changes to the main branch (through whatever approval process is in place) and hit the publish button. This published just the currently approved material, plus that writer's changes. None of the work in progress in the other writer's branches is included in the published version until it is merged with the main branch. The VCS detects an conflicts (two different authors updating the same content in different ways), triggering a process in which the conflicts are resolved by the appropriate writer. Writer can check content into their branches whenever they like and can add comments describing the changes they made. Every version that is check in is saved forever, so you can roll back any mistakes and track down who made what changes when at any time in the future. With a distributed VCS like Git, authors can take the whole repository with them of a trip and merge their changes back to the main repository when they get back.

        But this is all the VCS does. The build tools are separate. The workflow system is separate. The bug and feature request tracking system is separate. The editing function is separate. The validation and conformance checking is separate. This allows people to use best in class tools -- or the ones they like best -- for each of these functions. Nor does it create much in the way of integration issues (at least for this part of the process) since essentially each of these processes that needs to processed the source files does so on the file system (largely unaware of the fact that they are managed by a VCS). It is also typically much cheaper, since it uses tools already in use by the development team, most of which are open source and therefore free. 
        
        What a VCS does not do is to recreate the single portal to everything that people coming from a desktop-publishing background are used to, and that many CMS's take pains to emulate. 
        
        Another thing that the docs-as-code approach does not provide is the external {metadata} records that many CMS's provide, or the dependency tracking that they may provide, such as keeping track of which content is reused and where it is used. If you do a lot of ad-hoc reuse of document domain content with minimal internal metadata, these features may be essential to you. On the other hand, if you use subject-domain languages, or add an needed metadata to your files, and avoid creating too many ad-hoc dependencies (dependencies that are not based on the structure of the content itself) then you will not need these CMS features. 
        
                
    section: Buy vs Build
    

        The buy vs build decision is one of the most important you can make in implementing a system for any purpose. 
        
        But today it is seldom a straight dichotomy. Most of the things we buy, we customize to meet our needs. So it is really more a matter of buy and use the defaults vs buy and customize vs build. 
        
        But it is not as simple as that either. Today we are increasingly recognizing the virtue of using loosely coupled systems. That is, rather than using a single integrated tool to do the whole job, we use a tool chain: many small tools working in serial or in parallel with well defined interfaces between them. So the real choices are default vs customize vs integrate vs build. 
        
        When we integrate, we have the same choice with each of the tools we use to build our system: default vs customize vs integrate vs build. 
        
        Some of the tools we have discussed recognize this pattern and are themselves constructed of loosely coupled pieces so that you can make the default vs customize vs integrate vs build decision within that framework. This does not mean that all systems designed like this are equivalent, though. I matters very much how the divide their functionality into pieces, how cohesive each of the pieces is, how loose the coupling, and how adaptable the integration framework (the thing that allows you to string the pieces together) is. 
        
        When it comes to the cost of the tool itself, the order of preference is usually:
        
        default > customize > integrate > build
        
        In other words, unless a tool is outrageously expensive off the shelf, it is usually cheaper to buy and use the defaults and progressively more expensive to customize, integrate, and build. 
        
        But this if far from telling the whole story. While you should obviously try to find a tools whose defaults are closest to what your organization needs, off the shelf tools obviously have very generic defaults. In terms of the ability to meet your needs precisely, the order of preference is:
        
        build > integrate > customize > default 
        
        In other words, in order to meet your needs precisely, unless you are very lucky, the defaults of an off the shelf system will not suit your organization particularly well, and you will be able to meet them progressively better if you customize, integrate, and build. 
        
        Working around the limitations of generic system defaults creates costs in terms of inefficiency and lost quality. A system that meets you needs better will generally cost less to run and will help you make more money. Of course, many people and many organizations do choose to work with the defaults or to mildly customize off the shelf tools because while they may be inefficient compared to a custom solution, creating a custom solution not only involves greater costs, it is also outside their range of expertise. Indeed, not knowing how to create and maintain a custom tools set is probably a more compelling reason to stick to off the shelf tools than the costs of building. 
        
        On the other hand, as we said from the beginning, all writing you do with digital looks is structured writing. When we talk about moving to structured writing, what we mean is adding more structure to our writing in order to better serve a business need. At a certain point in this progression, you pass the point at which the added structure you need can be found off the shelf. At this point, adding more structure, particularly structure which is specific to your business needs, structure which enforces or factors out constraints that are important to your process efficiency and content quality, means moving in the direction of customization, integration, and building.
        
        Indeed, whether intended or not, many organizations find themselves going down this road. There is a pattern in many organizations where the initial decision is to buy off the shelf and use defaults but over time the need to better meet organizational needs leads to customization, then to integration with other tools, and finally to building some pieces themselves. Whether or not the original plan was to stick with defaults, needs and circumstances push people down the customize, integrate, build path. 
        
        The upside of this approach is that is is gradual and so is based on what the organization has learned about their needs over time. The downside is, the original tools were not chosen with ease of integration in mind. The integrations that get done are often ad hoc and brittle. Over time, the system can become fragile and it can be difficult to do further customization, integration, or building without breaking something. 
        
        No tooling lasts forever. One of the lessons we should learn from this is that content that is tied to a tool is going to create large problems for us when we are inevitably forces to change tools. Tool independence is often touted as the benefit of moving to structured writing, or more specifically of moving to XML. However, it is not the markup format that makes content independent of tools. If you use a format that has a lot of management domain markup, for instance, that ties your content that to tools that support those management semantics. DITA content, for instance, is portable between different DITA tools, but not to non-DITA tool chains. Pure document domain and pure subject domain formats have a much greater degree of independence. 
        
        Then again, we should not raise tool independence above all other considerations. Migrating content between tools is a cost, and a particularly thorny one because you often need to do it in a hurry in order to maintain production while executing a tool change. But maintaining tool independence above all other considerations may not be the most cost effective approach if you give up the specific business benefits of all other tools. 
        
        The default, customize, integrate, build decision is one that is filled with bogeymen. People will tell you never to do this or never to do that. But in the end it all comes down to cost benefit analysis. You can get any approach right and any approach wrong. Implementing any strategy badly can lead to disaster, but so can choosing the wrong strategy and implementing it well. 
        
   
        The real crunch point of any tool strategy, no matter which parts of the tool set you buy, customize, or build, is integration. Integration means getting all the pieces to work together without breaking. If integration fails, everything fails. 
        
        Integration is every bit as much about getting the markup language design right as it is about getting the tools right. 
        
        The simpler a system is, the easier the integration will be. Simplicity is not just about the number of pieces. It is much more about the simplicity of the relationships and information flows. A system in which all the information flow point in the same direction and in which there are minimal dependencies between pieces will the the easiest to integrate. 