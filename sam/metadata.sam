chapter:(#chapter.metadata) Metadata

    <<<(annotations.sam)

    We live in the age of metadata, so much so that the word metadata has almost come to replace the word data itself and has come to be applied to almost any form of data that describes a resource. For example, we hear a lot about law enforcement getting access to metadata related to phone calls, which simply means the data about which number called which number and for how long.  
    
    The standard definition of metadata is data that describes data, but that definition misses the central point. Metadata does not merely describe data, metadata creates data. Metadata turns an undifferentiated set of values into useful data by constraining the interpretation of those values. 

    In the case of our recipe example, we have seen that the ingredients of a recipe in a document domain markup are just items in a list. 
    
    ```(sam)
        section: Ingredients
            * 12 eggs
            * 2 qt water
        
    Adding subject domain markup allows us to clarify to algorithms exactly what these string. 
    
    ```(sam)

        ingredients:: ingredient, quantity, unit
            eggs, 12, each
            water, 2, qt
    
    This subject domain markup is metadata, and it turns what were just list items into a set of ingredient data. It is not the the data existed and the metadata came along afterward to describe it. It is that the data exists as data only because the metadata is there to describe it. Take away the metadata and data becomes noise. We (as designers of algorithms) simply do not know what it means. 
    
    In the {definition of structured writing}[#chapter.definition] I said that structured writing is writing that obeys constraints and that records the constraints it obeys. In other words, structured writing is the application of metadata to content which turns content into data by constraining the interpretation of the content. If structured writing is writing that obeys constraints and records the constraints it obeys,  Metadata is the means by which those constraints are recorded. Without it, you have no formal assurance that the information is what it seems to be. 
    
    This is not to say that a list structure is not metadata also. It is. It is {document domain} metadata. It formally identifies a piece of text as a list item. This makes it easy to write algorithms that recognize lists. This  allows you to reliably format list items in whatever media you choose to publish in. That metadata still transforms content into data, but not into ingredient data, just list item data. The operations we can perform on list item data are obviously far less sophisticated than those we can perform on ingredient data.  

    There is a very important point here: The same set of values, the same string of letter, words, and numbers, can be turned into different kinds of data by applying different kinds of metadata to them. This is a very clear illustration of the fact that it is metadata that creates data. And what it tells us is that we can choose what kind of data we want to turn our content into by choosing what type of metadata we want to apply to it.     
        
    Basic {publishing} algorithms can be performed using {document domain} metadata alone. To publish a list of ingredients it is enough to know that the string "6 eggs" is a list item. If we want to present ingredients different in different media, we need to know that "eggs" is an ingredient and "6" is quantity of that ingredient. But as we have seen, publishing-related algorithms such as {differential single sourcing}, {content reuse}, and {content generation} also require {subject domain} metadata. Establishing the {relevance} of content requires {subject domain} metadata because we search for content based largely on its subject matter. {Validating}(algorithm "conformance"), which deals with whether content meets its defined constraints, depends on the metadata of the domain in which the content is recorded. {Auditing}, which deals with whether content meets business requirements, generally requires {subject-domain} metadata as most business requirements are in the {subject domain}.  
 
    section: The recursive nature of metadata
    
        Metadata is confusing because it is recursive. If metadata is the data that describes (and thereby creates) data, it is also the data itself. And since data is created by metadata, this means that metadata is itself created by metadata. In other words, if the line `ingredients:: ingredient, quantity, unit` turns the ingredient lines into metadata in this markup: 
        
        ```(sam)
            ingredients:: ingredient, quantity, unit
                eggs, 12, each
                water, 2, qt
            
        Then what makes `ingredients:: ingredient, quantity, unit` a piece of metadata and not just another string of characters? Whatever it is, it is also metadata, and the content depends on that metadata as well, since it cannot exist without its metadata and that metadata cannot exist without the metadata that defines it. 
            
        In structured writing, we add structure to content to replace the things we have factored out. That structure is metadata to the data that is the text of the file. But if we store that file in any kind of repository, the information that identifies the file in that repository is metadata to the file as a whole. But that is not the only kind of metadata for that file. If the structure of the file is described by a {schema}, the schema is also metadata for the file. 
                
        But we're not done yet because the specification of the schema language is the metadata that tells you what the schema means.  And then of course, there is the specification of the markup to consider. The XML specification is part of the metadata for every XML document in existence. And we are still not be done, because the XML specification uses a formal grammar description language, called {BNF}(language). The BNF specification is metadata for the schema language description. 
        
        How do we break out of this infinite recursion of metadata. Data is information that has been formalized for interpretation by algorithms. Fortunately, human beings can understand natural language without that degree of formalization. Eventually, then, we reach a point where the last piece of metadata is not described by metadata but by human language. That human language document essentially bootstraps the whole metadata cascade that eventually yields pieces of data that can be unambiguously interpreted by algorithms. 
        
        So, every piece of data has a spreading tree of metadata supporting it, which, if traced to its roots, eventually leads to plain language documents that explain things in human terms.
        
        
    section: Where should metadata live?
        
        One of the great questions about metadata is where it should live: with the data it describes or separate from it? For much metadata the answer is obvious. It lives separately because its scope is wider than one resource. An XML document does not include the XML spec, for instance. But for metadata that is unique to an individual resource, the question is an important one. 

        Most early graphic file formats only stored the image. Most modern format also store extensive metadata about the image. The pictures you take with your digital camera include lots of information about the camera and the settings that were used to take the shot, and, if the camera has a GPS receiver, even the geographic coordinates of where the shot was taken. Having that metadata embedded in the file ensure that the picture and its metadata stay together. 

        In part the argument is about who is responsible for creating the metadata. In the case of the photo, the metadata is in the file because the camera is the best placed instrument to record it. Sometimes it is about whether the metadata is intrinsic or extrinsic to the content. Sometimes it is about control.

        For example, should the history of a file be stored in the file or in the repository? Storing it in the file lessens the file's dependence on the repository and makes it more portable. But a repository vendor may prefer to sell you a system in which to uninstall their repository would be to loose all your file history. If file status information is only stored in a workflow system, for instance, it is very hard to move away from that system. It it is stored in the file, it is easy to move away, and also to edit when not connected to the system, which can save you on licenses. 
        
        Writing your content in the subject domain means that more of your metadata is stored in the same file as the content, increasing its independence and portability. Also, as we have seen, the use of subject domain structures can lessen the need for management domain structures for algorithms like {single sourcing} and {content reuse}, which reduces the need for external management domain metadata. All of this contributes to improved functional lucidity, referential integrity, and change management.


        
        There is another way to apply {subject domain} metadata to content besides recording the content in the subject domain. This is to apply subject domain metadata externally to the content or an an overlay to the document domain structures in which the content is recorded. 
        
        For example, if you wanted to have access to the data on which ingredients were mentioned in each recipe (perhaps as an aid to finding recipes), you could record the recipe itself in the document domain and then place it in a relational database table with a many-to-many relationship to a table that listed all known ingredients.
        
        # Example
        
        Systems designed primarily for document domain content creation, such as DITA, can include formalized structures for attaching subject domain metadata to document domain structures. In DITA, the primary such mechanism is the {subject schema}(concept "DITA subject schema").
               
        But the two approaches are not equivalent. They vary substantially in capability, implementation requirements and ease of use.
        
        For example, when we talked about {differential single sourcing}(concept) we saw that {subject domain} content could be transformed into radically different document domain structures for publication in different media. (A table for paper vs. an interactive widget online, for instance.) Attaching subject domain metadata to a piece of content that contains a document domain structure (such as a table) does not give you the capability to differentially single source that table to different structures for other media.
        
        One area in which external subject-domain metadata excels is {finding and filtering}(algorithm). You can certainly find and filter on subject domain content, but because there are many different subject domain structures, it can be non-trivial to figure out how to write the query expression that does the {finding and filtering} you require. By contrast, external subject domain metadata can be stored in normalized data structures that are consistent and therefore easier to write queries for. This consistency also allows tools and systems to put high-level interfaces over these query mechanisms for users that don't know how to write queries themselves. As such, the approach is widely favored in {content management} systems.
        
        On the other hand, there is a limit to how fine grained a {find and filter} operation can be with external metadata. If you want the advantage of external metadata stored in normalized structures for ease of querying, it can only discriminate down to the level of the pieces of content it is applied to. It can retrieve that whole content unit, but not pieces of it. 
        
        For {validation}(algorithm "conformance"), it comes down to a matter of what {constraints} you want to validate against. As we have seen, moving content to the subject domain allows us to factor out many of the constraints that we want to apply to the document domain and to express and enforce the constraints we want to apply to how the content treats its subject matter. If content quality is your primary reason for adopting structured writing, these advantages are hard to ignore. 

        For {auditing}, the situation is much the same. If you are looking to audit the consistency of your content, document domain metadata will not suffice, as we noted in [#chapter.conformance]. If you are looking to audit your coverage of subject matter, external subject domain metadata attached to document domain content will give you a high level look at your coverage, but tell you little about quality or how well subjects are being covered. 
        
        When it comes to ease of use, there are things in favor of each approach. If you have standardized document structures and standardized metadata structures, then users only have to learn those structures.
        
        On the other hand, separating subject metadata from the writing of the content itself means that writers have to do two separate jobs. Rather than creating the metadata as they write in the {subject domain}, they have to write in the {document domain} and then create separate subject domain metadata to describe the document they have just written. This creates more work for the writer, and has frequently proven error prone. 
        
        This can significantly impact the quality of both the content and the metadata. Because the document domain does not impose any constraints on how a subject is treated, it it quite easy to apply subject domain metadata to content that simply does not merit it. Since metadata is supposed to express a promise that certain constraints have been met, this creates a broken promise. 
        
        The fact is, promising that a constraint has been met without validating that it has actually been met is a recipe for broken promises. Attaching {subject domain} metadata externally to {document domain} structures always runs this risk. Subject-related constraints have not been factored out or enforced. There has not even been effective subject-domain guidance provided to the writer while {writing}(algorithm "authoring") the content. 
        
        I noted above that external subject domain metadata excels in the area of {finding} because it makes it easy to write queries on standardized metadata structures. But there is a large caveat to this, which is data quality. If the promises the metadata makes are not kept by the content itself, the value and reliability of searches is severely diminished. 
        
        At the end of the day, the value of any system is limited by the quality of its data. Content management systems often fail or underperform because of poor quality data caused by a poor fit between content and its metadata. In particular, there is a tendency for data quality to decline over time. Systems based on external subject-domain metadata need to be aware of the problem and vigilant to prevent it.
        
        We should also note that the two approaches are not mutually exclusive. The subject metadata used in both cases is essentially the same, since it describes the same subject. There is no reason, therefore, why you cannot take {subject domain} content, with its integrated metadata, and extract the metadata to an external metadata store. This way you can use the same query interface to find the content without giving up the validation and auditing capability of subject domain content or the finer-grained application of metadata that the subject domain enables. This approach can significantly improve the quality of the metadata in the system because all the subject domain guidance and constraints have been applied to the content before it is stored. Thus, as long as the content itself is correct, there cannot be a mismatch between what the metadata promises and what the content delivers.
        
        There are ease-of-use advantages to his approach as well. Since the CMS metadata is extracted from the content metadata, the writer does not have to enter the metadata separately from the content after is is written. If your take such an approach, however, it is vital to be very clear about which is the canonical source of the metadata. The subject-domain content is the canonical source and the only one that should ever be edited. The exported metadata in the CMS is a cache of the content metadata created for the sake of optimizing searches. Of course, if your CMS was based on an {XML database}(tool), you would not need to cache the metadata in relational tables; you could query it directly. 
        
        
    section: Taxonomy and controlled vocabulary
    
        With subject domain metadata, whether is is stored internally in the content or externally in a CMS, it is important to be consistent in how subjects are named. Of course, it is also important in the content itself that key subjects be named consistently across the content set to avoid confusion for readers. If you are supplying a large-scale top-down navigation scheme for your content, that scheme is also going to be concerned with the names of things, and with choosing the right terms to name the subjects so that readers can find them.  
        
        For all of these reasons, large scale content projects need to exercise some control over terminology. (Control of terminology is important for translation as well, but that is outside the scope of this book.) There are two parts to the terminology control problem: establishing terminology, and enforcing its use. 
        
        The principal difficulties in establishing terminology are:
        
        * Human being have a fairly small "use vocabulary" and we reuse words all the time. In safety critical functions like air traffic control or the operating room, we train everyone to use a special unambiguous vocabulary (which is generally undecipherable to the layperson) but for most uses, the words we want to control are likely to be used in multiple ways that are not easy to disambiguate formally. 
        
        * People in different fields (even within the same organization) often use different terms for the same concept. What the chef calls "pork", the farmer calls "pig". What the English call "boot", North Americans call "trunk". Trying to force everyone to use the same term means forcing them to say things that don't make sense in their own field. 
        
        * People in different fields (even within the same organization) often use the same words to mean different things. What the conference organizer calls a function is not what the programmer calls a function. What a programmer calls a function is (in more subtle ways) not what a mathematician calls a function.
        
        * People in one field may have ten terms that make fine-grained distinctions among things that people in another field lump together under a single term. For instance, programmers make a distinction between subroutines, functions, methods, and procedures. I know of one documentation project on which it was mandated that all of these should be called "routines". This was OK most of the time, but because a problem when function pointers were introduced into the product. (You can't use any of the other terms with "pointer".) Enforcing a single term obscured a distinction the turned out to matter.  

        In short, very little of our terminology is truly universal. The meaning of words changes depending on the context you use them in and the audience you are addressing. This is, in fact, at the core of how content works and why it is different from other forms of data. Content tells stories, and what words mean depends on the context in which they are used in the story. Stories are not as precise as formal data, but the only way we have to define formal data is with stories. This is why the spreading tree of metadata that supports and explains any point of data always ends with a human-language document. All of structured writing is an attempt to bring some small part of the orderliness and manageability of data to stories to improve the quality and consistency of stories so that we can communicate more effectively. 

        Thus terminology control is important, because it is key to making our structures and metadata work, but at the same time it must be done with care and sensitivity and a real sense of the limits within which it is possible to control the terms we use to tell stories. Good taxonomies always confine themselves to specific domain and define their terminology within the confines of those domains, but even within all but the most strictly controlled domain (such as the operating theater or the control tower) there are still commonly many shades of meaning on individual words which are only fully disambiguated by the story.
        
    section: Top-down vs. bottom-up terminology control
        
        You can manage terminology top-down or bottom up. The principle tools of top-down management are controlled vocabularies and taxonomies. A controlled vocabulary is essentially a list of terms and their proper usage within a specific domain. 
        
        A taxonomy is a more elaborate scheme for controlling and categorizing the name of things. Taxonomies are frequently hierarchical in nature, defining not only the terms for individual things but the names for the classes of things. Thus a taxonomy does not just list sparrows and blue jays and robins, it also classifies them as birds, and birds as animals, and animals as living things. A good taxonomy should be specific to the domain for which it is intended. (Blue Jays and Cardinals occupy a very different place in a baseball taxonomy than in a ornithological taxonomy.) As a classification scheme, a taxonomy may be used not only as the basis for controlling vocabulary, but also as a basis for top-down navigation of a content set. 
        
        Alternatively, you can control terminology from the bottom up. To do this, you use {subject domain} {annotations} in your content to highlight key terms and place the usage in the appropriate domain.
        
        ```(sam)
            In {Rio Bravo}(movie), {the Duke}(actor "John Wayne") plays 
            an ex-Union colonel.

        In the passage above, the annotations call out the fact that "Rio Bravo" is the name of a movie and that "the Duke" is the name of an actor called "John Wayne".
        
        This is taxonomic information. It places "Rio Bravo" in the class "movie" and "John Wayne" in the class "actor", with the added information that "the Duke" is an alternate term for the actor John Wayne. By placing these terms in these classes, it makes it clear that "Rio Bravo" in this context is not the Mexican name for what Americans call the Rio Grande or any of the several American towns named Rio Bravo, and that "the Duke" refers to John Wayne and not to The Duke of Wellington (who was often called by that nickname) or any of the possible meanings of "Duke". In other words, the taxonomy is embedded in the content. The {subject domain} internalized taxonomic metadata. 
        
        
        To appreciate why this might be useful, we can look at some of the difficulties of maintaining and enforcing  vocabulary constraints. 
        
        As mentioned above, constraining vocabulary is hard because the same term can mean different things in different contexts, and different terms can mean the same thing in different contexts. If you attempt to build a taxonomy from the top down it can be very difficult to anticipate the various meanings a term may have in different contexts. Even if you study existing texts, there is no guarantee that you will exhaust all the possibilities, and such searches are tedious and time consuming. 
        
        Secondly, even once you have defined your taxonomy from the top down, there is the question of how it is going to be enforced. You can require authors to use terms from the taxonomy, but how is that going to work? Do you expect them to carry the entire taxonomy around in their heads? And as they are writing, do you expect that they will recognize that they are using a word not on the taxonomy every time they refer to a subject covered by the taxonomy? Any attempt to comply with such requirements is going to create a lot of mental overhead for writers, and, as we have noted ([#chapter.authoring]) dividing author's attention has a negative impact on content quality. 
        
        There are mechanical solutions that attempt to catch vocabulary problems, but the fact that words can mean so many things in different contexts means no such process can get it right all the time. 
        
        An alternative to first trying to define and then trying to an enforce a taxonomy from the top down it to let it emerge, in a disciplined way, from the content itself. The key to this is that in structured writing, particularly in the subject domain, you annotate those things that are significant to your content. If the vocabulary you are trying to enforce is not vocabulary that is significant to your content, you are probably wasting your time, so the vocabulary you want to use and the subjects you want to annotate should overlap. 
        
        If you have writers annotate the significant subjects in their content as they write, they will be annotation those very terms whose vocabulary you want to control. So when they mention the name of a bird like Blue Jay or Robin, they annotate it as `{blue jay}(bird)` or `{robin}(bird)`.
        
        Of course, you will sometimes need to revisit the list of all birds mentioned in your content to check them for accuracy and consistency. To get the current list, you simply have an algorithm scan your content an create a list of all the birds named in the content.  
        
        If an author mentions a new bird that you would not have thought to include in your taxonomy, it will get added to the list. You can have an algorithm alert you ever time a new bird is mentioned by an author. Effectively, now, your taxonomy is bubbling up from your content. Your authors are not having to worry about whether the terms they are using are in the taxonomy or not, as long as they mark up what type of thing they are naming. But your will still get an alert every time a new term is added, and will be able to evaluate if it is being used correctly.

        It is possible that some authors will forget to annotate some birds, or will annotate them incorrectly (as something other than a bird). But we can easily catch most of these mistakes as well. Incorrect annotations will tend to show up as anomalous entries in other annotation categories. In the cases where there are genuine name conflicts between two different domains, you can make a list of such conflicting names and use and algorithm to compile a list of all tagged instances of those names to review for incorrect tagging. For failure to tag at all, you can use the generated list of tagged terms to compile a list of terms to check for and periodically scan the content set to pick up significant cases. 
        
        Another useful {conformance} tool that can come out of the bottom-up approach is a stop list. A stop list is a list of terms that should not be used, but frequently are. It can be used by an algorithm to scan content for inappropriate vocabulary. Stop lists can only really be created bottom up. You can't anticipate or ban ever term anyone might ever come up with. You should only ban terms that are both problematic and which occur frequently. (The chance of false hits -- of banning terms that are perfectly legitimate in other contexts -- rises with every word you add to the stop list.) With a bottom up approach to terminology control, you get an accurate measure of which terms are being misused, and the frequency and nature of the misuse. This is an excellent basis for compiling a useful stop-list. 
        
        Also, because subject annotation can specify the type of a term (that is distinguish between `{Blue Jays}(baseball-team)` and `{Blue Jays}(bird)` you can make your stop list type specific: banning works used in one sense but not another. This can greatly reduce false hits, which is important because people tend to abandon the use of conformance tools if they produce very many false hits, as we can see from the very infrequent use of the grammar checkers in word processors. 

        As your taxonomy develops from the bottom up, it may be tempting to start to impose it from the top down, but this is probably not a good idea (though there may be exceptional circumstances). Having to comply with an top-down taxonomy while writing divides the writer's attention and compromises functional lucidity, both of which are bad for content quality and the author's efficiency. Continuing to rely on feedback to catch errors after initial authoring allows writers to stay in the moment as they write. If the feedback is given to the authors and they fix the errors themselves, they will learn the correct terminology and tagging over time and will make fewer mistakes. However, the door will remain open for new terms to be allowed into the content as appropriate, and to bubble up to the taxonomy, rather than banning them from the content until the taxonomy can be updated from the top down. 
        
        This approach does not make all vocabulary constraint problems go away, but it does have a number of advantages. 
        
        * It turns an up-front taxonomy development effort into an permanent part of the content development process. This not only reduces the spike in effort, it means that the taxonomy is based on real experience writing real content and that it is continually maintained as subjects and business objectives change. Indeed, if you are already annotating your content  to support any of the other structured writing algorithms, you essentially get taxonomy development, maintenance, and control almost for free. (Someone does have to review the reports, make the edits, and add to the canonical taxonomy, of course.)
        
        * It improves {functional lucidity} by not forcing authors to refer to the taxonomy while writing. If you are already annotating for other reasons, you are imposing no additional burden on authors at all. 
        
        * By improving the consistency of the annotations, it makes all the other algorithms that rely on them more reliable as well. (This is one of the greatest virtues of the subject domain. Subject domain markup can serve multiple algorithms, meaning you get the benefits of multiple algorithms with less cost. 
        
    section: Local and global names

        A big part of functional lucidity is the ability to call things by the names that are familiar to you. We noted in the discussion of the {relevance algorithm} ([#chapter.relevance]) that the use of clearly labeled unambiguous identifiers can help algorithms determine the relevance of a piece of content. The problem is, not everything has a globally unambiguous identifier (such as a company stock ticker) and even for things that do, the author may know know what those identifies are off the top of their heads (for instance, many authors may know the names of companies like Apple, Google, and Microsoft but not know their full ticker symbol). Forcing them to look them up every time you want to to unambiguously identify a company will add a lot of overhead to the authoring process, and it will also make the markup of the information more complex, again impacting the efficiency of authoring. 

        Fortunately, we don't have to unambiguously identify everything we write about at a global scale. We only have to ambiguously identify it within the context of the content we are writing. There is actually a universal truth about language in this. Very few words have only one meaning or identify only one thing. "Sun" is a big ball of burning gas and a computer company that was bought by Oracle. We distinguish these meaning by context, and we can distinguish content identifiers by context in the same way. 
        
        In fact, doing the identification in context is actually more accurate, since there is less possibility of accidentally introducing a confusion with a usage you are not aware of. The more highly contextualized an identifier is, the less ambiguous it is (as long as you specify both the context and the identifier you are looking for).   
        
        
        Rather than forcing the author to add this additional namespace information when they write (the equivalent of calling everyone by their full name and address around the dinner table), we can have the synthesis algorithm add it based on what it knows about the source of the content. This simplifies the author's task, which means they are more likely to provide the markup we want. (It is also another example of factoring out invariants, since we know that all method names in this particular piece of content will belong to the same API.)
            
        The namespace of a person's name is implicit in where they live. It is not that people lack a last name or address when they are at home. It is that we don't use those those additional identifiers because they are cumbersome and the first name is sufficiently unique to establish who we are talking about. Similarly, content created within a domain has its namespaces implicit within the domain in which it is created. 
        
        When we pull content from multiple domains, however, as we might do for purposed of {content reuse} or simply because we have divided authoring of a large content set into multiple domains to make it easier to create and manage, we need to make the namespaces of all the local names we use explicit in the content as we pull it out of the local domain. This can be done by algorithm because the algorithm can know which domain each pieces of content is coming from. 

            
        
    section: Ontology
        Finally it is worth saying a word about ontology. Ontology (in the information processing sense) is an attempt to create a formal mapping of the relationships between entities in the real world such that algorithms can draw inferences and reach conclusions about them.  
        
        In many way, therefore, an ontology is an attempt to do for algorithms what content does for humans. After all, one of the main reasons that we read is so that we can understand the world better, understand what various objects and institutions are and how they relate to each other, statically and in action, so that we can decide what to do. 
        
        In some sense, therefore, ontology is the ultimate in {subject domain} markup. And just as {subject domain} markup can be used to drive content management functions, so there are attempts to use ontologies to drive content management functions, especially at the enterprise level. One should also be able to generate human-readable content from an ontology, given a sufficiently sophisticated algorithm and a sufficiently sophisticated ontology. 
        
        All of this is very much outside our scope in this book. {Subject domain} markup is an attempt to capture certain aspects of the subject matter of a work. But it is not an attempt to model the argument of a work. Consider the passage:
        
        ```(sam)
            In {Rio Bravo}(movie), {the Duke}(actor "John Wayne") 
            plays an ex-Union colonel.
            
        Here the subject domain markup formalizes the fact that Rio Bravo is a movie and that "the Duke" is a reference to the actor John Wayne. It does not model the relationship between the two. An ontology would want to model the "starred in" relationship between John Wayne and Rio Bravo, whereas  {subject domain} structured writing is normally content to leave this to the text. 
        
        Similarly, this {subject domain} markup does not bother to denote that Union is a reference to both a country and its armed forces, and that colonel is a rank in those armed forces. It does not denote these things because this particular markup language is concerned with movies and these facts are entirely incidental to the movie business. Actors, directors, and movies are significant subjects in the movie review domain. The names of nations and their armies that figure in the plot of individual movies are incidental in that domain. A full ontological treatment of the passage above, however, would need to model those relationships. 
        
        Structured writing does make certain aspects of content clear to algorithms, but not with the intention of making it possible for the algorithms to make real-world inferences and decisions based on the information in that content. It only does what is necessary to allow human authors to use algorithms as tools to improve the quality of the content they prepare for human readers. 
