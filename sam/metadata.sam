chapter:(#chapter.metadata) Metadata

    <<<(annotations.sam)

    We live in the age of metadata, so much so that the word metadata has almost come to replace the word data itself and has come to be applied to almost any form of data that describes a resource. For example, we hear a lot about law enforcement getting access to metadata related to phone calls, which simply means the data about which number called which number and for how long.  
    
    In the {definition of structured writing}[#chapter.definition] I said that structured writing is writing that obeys constraints and that records the constraints it obeys. The standard definition of metadata is data that describes data. This can make it seem like an optional extra, as something added on top of something that already exists. But actually there is no data without metadata. Data is a formalization of information. It makes a set of promises about what the information means -- that is, how it is constrained. Metadata is the record of those promises/constraints. Without it, you have no formal assurance that the information is what it seems to be. 
    
    Structure is the imposition and recording of constraints. The recording of constraints is metadata. Structure, therefore, consists of metadata. Where you have structure, you have metadata. Where you have metadata, you have structure. 
    
    Recipes contain lists of ingredients. If you record a recipe in the document domain, you will record that list of ingredients using a list structure. In this case the constraint you are imposing, and therefore the metadata you are recoding is: this is a list and these are list items. This metadata does not record that the items in the list are ingredients. Human beings recognize the list of ingredients for what it is based on familiarity with the pattern and recognition of the names of foodstuffs. This is fine for humans, but still a little iffy for our current generation of AI algorithms. Writing an algorithm to reliably recognize ingredients in recipe, while not impossible, is a lot of work.
    
    Move that recipe into the {subject domain}, however, and you replace a list structure with an ingredients structure. In this case the constraint you are imposing, and therefore the metadata you are recording, is: this a a collection of ingredients, and each one of them is an ingredient. Writing a reliable algorithm to recognize ingredients based on explicit metadata that says they are ingredients is simple. 
    
    This is not to say that a list structure is not metadata also. It is. It is {document domain} metadata. It formally identifies the list items structure as a list item. It makes it easy to write algorithms that recognize lists. This  allows you to reliably format it as a list item in whatever media you choose to publish it in. 
    
    Subject domain metadata that identifies an ingredient as an ingredient provide a different promise, one that allows us to do more with the data. 
    
    section: Metadata enables algorithms
        
        The purpose of attaching metadata to content is to make the content into data. The reason for making content into data is to make it accessible to algorithms. The best way to think about metadata is in terms of the algorithms it enables. 
        
        Basic {publishing} algorithms can be performed using {document domain} metadata alone. But as we have seen, publishing-related algorithms such as {differential single sourcing}, {content reuse}, and {content generation} also require {subject domain} metadata. Establishing the {relevance} of content requires {subject domain} metadata because we search for content based largely on its subject matter. {Validating}(algorithm "conformance"}, which deals with whether content meets its defined constraints, depends on the metadata of the domain in which the content is recorded. {Auditing}, which deals with whether content meets business requirements, generally requires {subject-domain} metadata as most business requirements are in the {subject domain}.  
        
        But this does not mean that all content gets captured in the {subject domain}. There is another way to apply {subject domain} metadata to content besides recording the content in the subject domain. This is to apply subject domain metadata externally to the content or an an overlay to the document domain structures in which the content is recorded. 
        
        For example, if you wanted to have access to the data on which ingredients were mentioned in each recipe (perhaps as an aid to finding recipes), you could record the recipe itself in the document domain and then place it in a relational database table with a many-to-many relationship to a table that listed all know ingredients.
        
        # Example
        
        Systems designed primarily for document domain content creation, such as DITA, can include formalized structures for attaching subject domain metadata to document domain structures. In DITA, the primary such mechanism is the {subject schema}(concept "DITA subject schema").
        
        One of the primary contrasts between the document domain and the subject domain then is not that subject domain metadata is entirely missing from the document domain, but that in document domain systems, subject domain metadata is stored separately from the document domain markup, whereas in the subject domain it is integrated into the content itself. 
        
        But the two approaches are equivalent. They vary substantially in capability, implementation requirements and ease of use.
        
        For example, when we talked about {differential single sourcing}(concept) we saw that {subject domain} content could be transformed into radically different document domain structures for publication in different media. (A table for paper vs. an interactive widget online, for instance.) Attaching subject domain metadata to a piece of content that contains a document domain structure (such as a table) does not give you the capability to differentially single source that table to different structures for other media.
        
        One area in which external subject-domain metadata excels is {finding and filtering}(algorithm). You can certainly find and filter on subject domain content, but because there are many different subject domain structures, it can be non-trivial to figure out how to write the query expression that does the {finding and filtering} you require. By contrast, external subject domain metadata can be stored in normalized data structures that are consistent and therefore easier to write queries for. This consistency also allows tools and systems to put high-level interfaces over these query mechanisms for users that don't know how to write queries themselves. As such, the approach is widely favored in {content management} systems.
        
        On the other hand, there is a limit to how fine grained a {find and filter} operation can be with external metadata. If you want the advantage of external metadata stored in normalized structures for ease of querying, it can only discriminate down to the level of the pieces of content it is applied to. It can retrieve that whole content unit, but not pieces of it. 
        
        For {validation}(algorithm "conformance"), it comes down to a matter of what {constraints} you want to validate against. As we have seen, moving content to the subject domain allows us to factor out many of the constraints that we want to apply to the document domain and to express and enforce the constraints we want to apply to how the content treats its subject matter. If content quality is your primary reason for adopting structured writing, these advantages are hard to ignore. 

        For {auditing}, the situation is much the same. If you are looking to audit the consistency of your publishing system, document domain metadata will suffice. If you are looking to audit your coverage of subject matter, external subject domain metadata attached to document domain content will give you a high level look at your coverage, but tell you little about quality or how well subjects are being covered. 
        
        When it comes to ease of use, there are things in favor of each approach. If you have standardized document structures and standardized metadata structures, then users only have to learn those structures.
        
        On the other hand, separating subject metadata from the writing of the content itself means that writers have to do two separate jobs. Rather than creating the metadata as they write in the {subject domain}, they have to write in the {document domain} and then create separate subject domain metadata to describe the document they have just written. This create more work for the writer, and has frequently proven error prone. 
        
        This can significantly impact the quality of both the content and the metadata. Because the document domain does not impose any constraints on how a subject is treated, it it quite easy to apply subject domain metadata to content that simply does not merit it. If metadata is supposed to express a promise that certain constraints have been met, this creates a broken promise. 
        
        The fact is, promising that a constraint has been met without validating that it has actually been met is a recipe for broken promises. Attaching {subject domain} metadata externally to {document domain} structures always runs into this problem. Subject-related constraints have not been factored out or enforced. There has not even been effective subject-domain guidance provided to the writer while {writing}(algorithm "authoring") the content. 
        
        I noted above that external subject domain metadata excels in the area of {finding} because it makes it easy to write queries on standardized metadata structures. But there is a large caveat to this, which is data quality. If the promises the metadata makes are not kept by the content itself, the value and reliability of searches is severely diminished. 
        
        At the end of the day, the value of any system is limited by the quality of its data. Content management systems often fail or underperform because of poor quality data caused by a poor fit between content and its metadata. In particular, there is a tendency for data quality to decline over time. Systems based on external subject-domain metadata need to be aware of the problem and vigilant to prevent it.
        
        We should also note that the two approaches are not mutually exclusive. The subject metadata used in both cases is essentially the same, since it describes the same subject. There is no reason, therefore, why you cannot take {subject domain} content, with its integrated metadata, and extract the metadata to an external metadata store. This way you can use the same query interface to find the content without giving up the validation and auditing capability of subject domain content or the finer-grained application of metadata that the subject domain enables. This approach can significantly improve the quality of the metadata in the system because all the subject domain guidance and constraints have been applied to the content before it is stored. Thus, as long as the content itself is correct, there cannot be a mismatch between what the metadata promises and what the content delivers.
        
        There are ease-of-use advantages to his approach as well. Since the CMS metadata is extracted from the content metadata, the writer does not have to enter the metadata separately from the content after is is written. If your take such an approach, however, it is vital to be very clear about which is the canonical source of the metadata. The subject-domain content is the canonical source and the only one that should ever be edited. The exported metadata in the CMS is a cache of the content metadata created for the sake of optimizing searches. Of course, if your CMS was based on an {XML database}(tool), you would not need to cache the metadata in relational tables; you could query it directly. 
        
    section: The recursive nature of metadata
    
        Metadata is confusing because it is recursive. If metadata is the data that describes data, it is also the data that describes the metadata. The data/metadata distinction is not one of type -- metadata is not a different kind of thing from data -- it is one of relationship. It is a father/son distinction, not a cat/dog distinction. One man can be both a father and son. What we call him depends on what relationship we are looking at at a given moment. 
        
        In structured writing, we add structure to content to replace the things we have factored out. That structure is metadata to the data that is the text of the file. But if we store that file in any kind of repository, the information that identifies the file in that repository is metadata to the file as a whole. But that is not the only kind of metadata for that file. If the structure of the file is described by a {schema}, the schema is also metadata for the file. 
                
        But we're not done yet because the specification of the schema language is the metadata that tells you what the schema means.  And then of course, there is the specification of the markup to consider. The XML specification is part of the metadata for every XML document in existence. And we are still not be done, because the XML specification uses a formal grammar description language, called {BNF}(language). The BNF specification is metadata for the schema language description. 
        
        So, every piece of data has a spreading tree of metadata supporting it, which, if traced to its roots, eventually leads to plain language documents that explain things in human terms.
        
        One of the great questions about metadata is where it should live: with the data it describes or separate from it? For much metadata the answer is obvious. It lives separately because its scope is wider than one resource. An XML document does not include the XML spec, for instance. But for metadata that is unique to an individual resource, the question is an important one. 

        Most early graphic file formats only stored the image. Most modern format also store extensive metadata about the image. Your pictures you take with your digital camera include lots of information about the camera and the settings that were used to take the shot, and even the geographic coordinates of where the shot was taken, if the camera has a GPS receiver. Having that metadata embedded in the file ensure that the picture and its metadata stay together. 

        In part the argument is about who is responsible for creating the metadata. In the case of the photo, the metadata is in the file because the camera is the best placed instrument to record it. Sometimes it is about whether the metadata is intrinsic or extrinsic to the content. Sometimes it is about control.

        For example, should the history of a file be stored in the file or in the repository? Storing it in the file lessens the file's dependence on the repository and makes it more portable. But a repository vendor may prefer to sell you a system in which to uninstall their repository would be to loose all your file history. If file status information is only stored in a workflow system, for instance, it is very hard to move away from that system. It it is stored in the file, it is easy to move away, and also to edit when not connected to the system, which can save you on licenses. 
        
        Writing you content in the subject domain means that more of your metadata is stored in the same file as the content, increasing its independence and portability. Also, as we have seen, the use of subject domain structures can lessen the need for management domain structures for algorithms like {single sourcing} and {content reuse}, which reduces the need for external management domain metadata.
        
    section: Taxonomy and controlled vocabulary
    
        With subject domain metadata, whether is is stored internally in the content or externally in a CMS, it is important to be consistent in how subjects are named. Of course, it is also important in the content itself that key subjects be named consistently across the content set to avoid confusion for readers. If you are supplying a large-scale top-down navigation scheme for your content, that scheme is also going to be concerned with the names of things, and with choosing the right terms to name the subjects so that readers can find them. (Small scale top-down navigation schemes may work on a different principle, such as presenting a curriculum to the reader, but on an scale in which it is not possible to predict or specify a single reading order, at at which simple alphabetical listing breaks down, classification of the content it really the only way to provide top-down navigation.) 
        
        For all of these reasons, large scale content projects need to exercise some control over terminology. (Control of terminology is important for translation as well, but that is outside the scope of this book.) There are two parts to the terminology control problem: establishing terminology, and enforcing its use. 
        
        The principal difficulties in establishing terminology are:
        
        * Human being have a fairly small use vocabulary and we reuse words all the time. In safety critical functions like air traffic control or the operating room, we can train everyone to use a special unambiguous vocabulary (which is generally undecipherable to the layperson) but for most uses, the words we want to control are likely to be used in multiple ways that are not easy to disambiguate formally. 
        
        * People in different fields (even within the same organization) often use different terms for the same concept. What the chef calls "pork", the farmer calls "pig". What the English call "boot", North Americans call "trunk". Trying to force everyone to use the same term means forcing them to say things that don't make sense in their own field. 
        
        * People in different fields (even within the same organization) often use the same words to mean different things. What the conference organizer calls a function is not what the programmer calls a function. 
        
        * People in one field may have ten terms that make fine-grained distinctions among things that people in another field lump together under a single term. For instance, programmers make a distinction between subroutines, functions, methods, and procedures. I know of one documentation project on which it was mandated that all of these should be called "routines". This was OK most of the time, but because a problem when function pointers were introduced into the product. (You can't use any of the other terms with "pointer".) And there are even differences in how these terms are used in different programming languages. 

        In short, very little of our terminology is truly universal. The meaning of words changes depending on the context you use them in and the audience you are addressing. This is, in fact, at the core of how content works and why it is different from other forms of data. Content tells stories, and what words mean depends on the context of the story in which they are used. Stories are not as precise as formal data, but the only way we have to define formal data is with stories. This is why the spreading tree of metadata that supports and explains any point of data always ends with a human-language document. All of structured writing is an attempt to bring some small part of the orderliness and manageability of data to stories to improve the quality and consistency of stories so that we can communicate more effectively. 

        Thus terminology control is important, because it is key to making our structures and metadata work, but at the same time it must be dome with care and sensitivity and a real sense of the limits within which it is possible to control the terms we use to tell stories. 
        
        You can define terminology top-down or bottom up. The principle tools of top-down definition are controlled vocabularies and taxonomies. 
        
        A controlled vocabulary is essentially a list of terms and their proper usage within a specific domain. 
        
        A taxonomy is a more elaborate scheme for controlling and categorizing the name of things. Taxonomies are frequently hierarchical in nature, defining not only the terms for individual things but the names for the classes of things. Thus a taxonomy does not just list sparrows and blue jays and robins, it also classifies them as birds, and birds as animals, and animals as living things. A good taxonomy should be specific to the domain for which it is intended. As a classification scheme, a taxonomy may be used not only as the basis for controlling vocabulary, but also as a basis for top-down navigation of a content set. 
        
        Alternatively, you can control terminology from the bottom up. To do this, you use {subject domain} {annotations} in your content to highlight key terms and place the usage in the appropriate domain.
        
        ```(sam)
            In {Rio Bravo}(movie), {the Duke}(actor "John Wayne") plays an ex-Union colonel.

        In the passage above, the annotations call out the fact that "Rio Bravo" is the name of a movie and that "the Duke" is the name of an actor called "John Wayne".
        
        This is taxonomic information. It places "Rio Bravo" in the class "movie" and "John Wayne" in the class "actor", with the added information that "the Duke" is an alternate term for the actor John Wayne. By placing these terms in these classes, it makes it clear that "Rio Bravo" in this context is not the Mexican names for the Rio Grande or any of the several American towns named Rio Bravo, and that "the Duke" refers to John Wayne and not to The Duke of Wellington (who was often called by that nickname) or any of the possible meanings of "Duke".
        
        By querying a content set that contains such annotation (and possible querying other subject domain information it contains as well) it is possible to extract a taxonomy from your content set. 
        
        To appreciate why this might be useful, we can look at some of the difficulties of maintaining and enforcing  vocabulary constraints. 
        
        As mentioned above, constraining vocabulary is hard because the same term can mean different things in different contexts, and different terms can mean the same thing in different contexts. If you attempt to build a taxonomy from the top down it can be very difficult to anticipate the various meanings a term may have in different contexts. Even if you study existing texts, there is no guarantee that you will exhaust all the possibilities, and such searches are tedious and time consuming. 
        
        Secondly, even once you have defined your taxonomy from the top down, there is the question of how it is going to be enforced. You can require authors to use terms from the taxonomy, but how is that going to work? Do you expect them to carry the entire taxonomy around in their heads? And as they are writing, do you expect that they will recognize that they are using a word not on the taxonomy every time they refer to a subject covered by the taxonomy? Any attempt to comply with such requirements is going to create a lot of mental overhead for writers, and, as we have noted ([#chapter.authoring]) dividing author's attention has a negative impact on content quality. 
        
        There are mechanical solutions that attempt to catch vocabulary problems, but the fact that words can mean so many things in different contexts means no such process can get it right all the time. 
        
        If, rather than creating a top-down taxonomy and then trying to force writers to use it, you have writers annotate the significant subjects in their content as they write. Now the job of scanning content disambiguating different usages of words is an ongoing part of the authoring process, so you don't have to put that same heavy effort into compiling the taxonomy, and that word is ongoing as long as the content set is in development, meaning that it will keep turning up new and interesting cases as the subject matter and your business goals develop.
        
        Of course, writers are not going to be consistent in the phrases they choose or how they annotate them. But you can use queries to pull out the annotations from the entire content set and sort them my type and content. This will tell you what names authors are giving to types and how they are naming things. You can then decide what the canonical terms should be and make the appropriate edits to the content set. 
        
        This means that you will start to build a canonical taxonomy outside the content set, simply as a way to record the terminology decision you have made. Authors may start to learn the correct terms, which will reduce the number of errors. But you don't need to force authors to look things up in the taxonomy while writing. Have them continue to annotate freely as before and develop and algorithms that will periodically check the annotations in the content set against the canonical taxonomy. (You could do this when a new or edited pieces of content is checked in, if you need to keep a tighter lease on the process.) Any terms that don't match are then reviewed to see if they need to be edited in the content or added to the canonical taxonomy. 
        
        This approach makes mechanical {conformance} testing much simpler and more reliable, since the algorithm is now reading well defined metadata in the text rather than trying to figure out the terminology being used in natural language text. 
        
        Of course, there is still the problem that authors may not annotate a term ever time that they should. But the list of terms that they do annotate becomes a checklist that you can use to have an algorithm scan the text to see if those terms should be annotated elsewhere. 
        
        Another things you can do is to use the annotation to develop a stop list. If authors are using the incorrect version of the company name, for instance, you can look at the list of annotations of type "company-name" and find the ones that are incorrect versions of your company name. You can then add those to a stop list so that they will be flagged immediately if they are used again (whether they are annotated or not). A stop list based on actual mistakes that people make is always the most effective. 
        
        This approach does not make all vocabulary constraint problems go away, but it does have a number of advantages. 
        
        * It turns an up-front taxonomy development effort into an permanent part of the content development process. This not only reduces the spike in effort, it means that the taxonomy is based on real experience writing real content and that it is continually maintained as subjects and business objectives change. Indeed, if you are annotation your content already to support any of the other structured writing algorithms, you essentially get taxonomy development, maintenance, and control almost for free. (Someone does have to review the reports, make the edits, and add to the canonical taxonomy, of course.)
        
        * It improves {functional lucidity} by not forcing authors to refer to the taxonomy while writing. If you are already annotating for other reasons, you are imposing no additional burden on authors at all. 
        
        * By improving the consistency of the annotations, it makes all the other algorithms that rely on them more reliable as well. (This is one of the greatest virtues of the subject domain. Subject domain markup can serve multiple algorithms, meaning you get the benefits of multiple algorithms with less cost. 
        
        
        
    section: Ontology
        Finally it is worth saying a word about ontology. Ontology (in the information processing sense) is an attempt to create a formal mapping of the relationships between entities in the real world such that algorithms can draw inferences and reach conclusions about them.  
        
        In many way, therefore, an ontology is an attempt to do for algorithms what content does for humans. After all, one of the main reasons that we read is so that we can understand the world better, understand what various objects and institutions are and how they relate to each other, statically and in action, so that we can decide what to do. 
        
        In some sense, therefore, ontology is the ultimate in {subject domain} markup. And just as {subject domain} markup can be used to drive content management functions, so there are attempts to use ontologies to drive content management functions, especially at the enterprise level. One should also be able to generate human-readable content from an ontology, given a sufficiently sophisticated algorithm and a sufficiently sophisticated ontology. 
        
        All of this is very much outside our scope in this book. {Subject domain} markup is an attempt to capture certain aspects of the subject matter of a work. But it is not an attempt to model the argument of a work. Given the passage:
        
        ```(sam)
            In {Rio Bravo}(movie), {the Duke}(actor "John Wayne") plays an ex-Union colonel.
            
        The subject domain is content to formalize the fact that Rio Bravo is a movie and that "the Duke" is a reference to the actor John Wayne. It does not model the relationship between the two. An ontology would want to model the "starred in" relationship between John Wayne and Rio Bravo, whereas the subject domain is content to leave this to the text. 
        
        Similarly, this {subject domain} markup does not bother to denote that Union is a reference to both a country and its armed forces, and the colonel is a rank in those armed forces. It does not denote these things because this particular markup language is concerned with movies and these facts are entirely incidental to the movie business. A full ontological treatment of the passage above, however, would need to model those relationships. 
        
        Structured writing does make certain aspects of content clear to algorithms, but not with the intention of making it possible for the algorithms to act on the information in that content. It only does what is necessary to allow human authors to use algorithms as tools to improve the quality of the content they prepare for human readers. 
