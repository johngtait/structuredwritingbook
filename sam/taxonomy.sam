chapter: Taxonomy and controlled vocabulary
    <<<(annotations.sam)
    index::type, term
        concept, taxonomy
        concept, controlled vocabulary

    Whether metadata is is stored internally in the content or externally in a CMS, it is important to be consistent in the terms used. You cannot constrain the interpretation of content if you don contain the metadata values you use to define it. Of course, it is also important in the content itself that key subjects be named consistently across the content set to avoid confusion for readers. If you are supplying a large-scale top-down navigation scheme for your content, that scheme is also going to be concerned with the names of things, and with choosing the right terms to name the subjects so that readers can find them.  
    
    For all of these reasons, large scale content projects need to exercise some control over terminology. (Control of terminology is important for translation as well, but that is outside the scope of this book.) Managing terminology is a complex problem, and it is one of those problem where a simplistic approach can result in lots of dropped complexity landing on the user in the form or incomprehensible language, incorrect classification, or poor connections between units content. The biggest danger here is thinking of taxonomy as a simple data management problem. Separating words from their role in sentences and paragraphs certainly makes them easier to fit into the traditional rows and columns of data management, which makes the problem look a lot simpler than it really is. Once you see it this way, it seems straightforward to get a bunch of people in a room, spin up a list of words and their definitions, and declare it as your corporate taxonomy, but this is a false partitioning of the problem. 
    
    The principal difficulties in establishing terminology are:
    
    * Human being have a fairly small "use vocabulary" and we reuse words all the time. In safety critical functions like air traffic control or the operating room, we train everyone to use a special unambiguous vocabulary (which is generally undecipherable to the layperson) but for most uses, the words we want to control are likely to be used in multiple ways that are not easy to disambiguate formally. 
    
    * People in different fields (even within the same organization) often use different terms for the same concept. What the chef calls "pork", the farmer calls "pig". What the English call "boot", North Americans call "trunk". Trying to force everyone to use the same term means forcing them to say things that don't make sense in their own field. 
    
    * People in different fields (even within the same organization) often use the same words to mean different things. What the conference organizer calls a function is not what the programmer calls a function. What a programmer calls a function is (in more subtle ways) not what a mathematician calls a function.
    
    * People in one field may have ten terms that make fine-grained distinctions among things that people in another field lump together under a single term. For instance, programmers make a distinction between subroutines, functions, methods, and procedures. I know of one documentation project on which it was mandated that all of these should be called "routines". This was OK most of the time, but because a problem when function pointers were introduced into the product. (You can't use any of the other terms with "pointer".) Enforcing a single term obscured a distinction the turned out to matter.  

    In short, very little of our terminology is truly universal. The meaning of words changes depending on the context you use them in and the audience you are addressing. This is, in fact, at the core of how content works and why it is different from other forms of data. Content tells stories, and what words mean depends on the context in which they are used in the story. Stories are not as precise as formal data, but the only way we have to define formal data is with stories. This is why the spreading tree of metadata that supports and explains any point of data always ends with a human-language document. All of structured writing is an attempt to bring some small part of the orderliness and manageability of data to stories to improve the quality and consistency of stories so that we can communicate more effectively. 

    Thus terminology control is important, because it is key to making our structures and metadata work, but at the same time it must be done with care and sensitivity and a real sense of the limits within which it is possible to control the terms we use to tell stories. Good taxonomies always confine themselves to specific domain and define their terminology within the confines of those domains, but even within all but the most strictly controlled domain (such as the operating theater or the control tower) there are still commonly many shades of meaning on individual words which are only fully disambiguated by the story.
    
    We have seen how we can use context to determine the meaning of block names in structured writing. Terms don't have to universally unique to be clear is we can identify them unambiguously in context. There that cannot easily be controlled universally can often be controlled in context. In other words, the terminology problem becomes much more tractable when it is partitioned appropriately. Structured writing is the ideal tool for partitioning the terminology problem by providing the context in which terms are understood. We have seen that annotations can be made more precise by identifying the type of subject being named. Here again we are adding context to vocabulary to allow us to control it better in its local domain. One of the virtues of the subject domain is that every subject domain content type and indeed every subject domain block type provides context for controlling the interpretation of vocabulary. (Which is, once again, an instance of using structured writing to constraint the interpretation of content.)
        
    section: Top-down vs. bottom-up terminology control
        
        There are two ways to partition the terminology control problem: top-down or bottom up. The principle tools of top-down management are controlled vocabularies and taxonomies. A controlled vocabulary is essentially a list of terms and their proper usage within a specific domain. 
        
        A taxonomy is a more elaborate scheme for controlling and categorizing the name of things. Taxonomies are frequently hierarchical in nature, defining not only the terms for individual things but the names for the classes of things. Thus a taxonomy does not just list sparrows and blue jays and robins, it also classifies them as birds, and birds as animals, and animals as living things. A good taxonomy should be specific to the domain for which it is intended. (Blue Jays and Cardinals occupy a very different place in a baseball taxonomy than in a ornithological taxonomy.) As a classification scheme, a taxonomy may be used not only as the basis for controlling vocabulary, but also as a basis for top-down navigation of a content set. 
        
        Alternatively, you can control terminology from the bottom up. To do this, you use {subject domain} {annotations} in your content to highlight key terms and place the usage in the appropriate domain.
        
        ```(sam)
            In {Rio Bravo}(movie), {the Duke}(actor "John Wayne") plays 
            an ex-Union colonel.

        In the passage above, the annotations call out the fact that "Rio Bravo" is the name of a movie and that "the Duke" is the name of an actor called "John Wayne".
        
        This is taxonomic information. It places "Rio Bravo" in the class "movie" and "John Wayne" in the class "actor", with the added information that "the Duke" is, in context, an alternate term for the actor John Wayne. By placing these terms in these classes, it makes it clear that "Rio Bravo" in this context is not the Mexican name for what Americans call the Rio Grande or any of the several American towns named Rio Bravo, and that "the Duke" refers to John Wayne and not to The Duke of Wellington (who was often called by that nickname) or any of the possible meanings of "Duke". In other words, the taxonomy is embedded in the content. The {subject domain} internalizes taxonomic metadata. 
        
        To appreciate why this might be useful, we can look at some of the difficulties of maintaining and enforcing  vocabulary constraints. 
        
        As mentioned above, constraining vocabulary is hard because the same term can mean different things in different contexts, and different terms can mean the same thing in different contexts. If you attempt to build a taxonomy from the top down it can be very difficult to anticipate the various meanings a term may have in different contexts. Even if you study existing texts, there is no guarantee that you will exhaust all the possibilities, and such searches are tedious and time consuming. 
        
        Secondly, even once you have defined your taxonomy from the top down, there is the question of how it is going to be enforced. You can require authors to use terms from the taxonomy, but how is that going to work? Do you expect them to carry the entire taxonomy around in their heads? And as they are writing, do you expect that they will recognize that they are using a word not on the taxonomy every time they refer to a subject covered by the taxonomy? Any attempt to comply with such requirements is going to create a lot of mental overhead for writers, and, as we have noted ([#chapter.authoring]) dividing author's attention has a negative impact on content quality. In other words, you are dumping a highly complex task that depends on a huge amount of data, onto each individual writer and requiring them to pay attention to it continuously in every word they write. This is the antithesis of good partitioning of complexity in the content system.
        
        There are mechanical solutions that attempt to catch vocabulary problems, but the fact that words can mean so many things in different contexts means no such process can get it right all the time. 
        
        An alternative to first trying to define and then trying to an enforce a taxonomy from the top down it to let it emerge, in a disciplined way, from the content itself. The key to this is that in structured writing, particularly in the subject domain, you annotate those things that are significant to your content. If the vocabulary you are trying to enforce is not vocabulary that is significant to your content, you are probably wasting your time, so the vocabulary you want to use and the subjects you want to annotate should overlap. 
        
        If you have writers annotate the significant subjects in their content as they write, they will be annotating those very terms whose vocabulary you want to control. So when they mention the name of a bird like blue jay or robin, they annotate it as `{blue jay}(bird)` or `{robin}(bird)`. By specifying the type of the subject you are partitioning the vocabulary in context.  
        
        Of course, you will sometimes need to revisit the list of all birds mentioned in your content to check them for accuracy and consistency. To get the current list, you simply have an algorithm scan your content an create a list of all the birds named in the content.  One role audits the data provided by another role, partitioning the taxonomy compliance task. 
        
        If an author mentions a new bird that you would not have thought to include in your taxonomy, it will get added to the list. You can have an algorithm alert you ever time a new bird is mentioned by an author. Effectively, now, your taxonomy is bubbling up from your content. Your authors are not having to worry about whether the terms they are using are in the taxonomy or not, as long as they mark up what type of thing they are naming. But your will still get an alert every time a new term is added, and will be able to evaluate if it is being used correctly. Postponing conformance checks is and effective way to partition conformance and put it in the hands of people who are taxonomy experts. That is, rather than forcing writers to comply with a complex and lengthy taxonomy as they write, a significant drain on their attention, you let them write using the terms that seem best to them, and have the taxonomy expert, aided by algorithms, validate the use of terminology. (It there are large or frequent differences between the official taxonomy and the language the writers are naturally using, this may indicate that the taxonomy is out of touch with how the subject matter is actually being described.)

        It is possible that some authors will forget to annotate some birds, or will annotate them incorrectly (as something other than a bird). But we can easily catch most of these mistakes as well. Incorrect annotations will tend to show up as anomalous entries in other annotation categories. In the cases where there are genuine name conflicts between two different domains, you can make a list of such conflicting names and use and algorithm to compile a list of all tagged instances of those names to review for incorrect tagging. For failure to tag at all, you can use the generated list of tagged terms to compile a list of terms to check for and periodically scan the content set to pick up significant cases. 
        
        Centralization and abstraction is not always the best form of partitioning. In content in particular, it is often better to partition in context -- which is what semantic annotation does. Partitioning tasks means sharing data. Partitioning data, therefore, can sometimes complicate tasks. (As with the location of metadata problem.)
        
        The point of partitioning and redistributing the complexity of a problem is to direct the complexity to those best placed to handle it. This sometimes means directing complexity to a central person or process which specialized or particular knowledge, such as the organizations formatting standards. But just as often it means distributing the complexity outward or down to people in the field who have the contextual knowledge needed to make correct decisions. Taxonomy is a third case, one in which only the writers in the field can tell you what ideas need to be expressed and only a central process can coordinate and disseminate information about what terms have been used and decided upon. Any process that does not allow communication in both directions, and does not allow writers to make good decisions in context, will end up dumping complexity on the reader. 
        
        Another useful {conformance} tool that can come out of the bottom-up approach is a stop list. A stop list is a list of terms that should not be used, but frequently are. It can be used by an algorithm to scan content for inappropriate vocabulary. Stop lists can only really be created bottom up. You can't anticipate or ban ever term anyone might ever come up with. You should only ban terms that are both problematic and which occur frequently. (The chance of false hits -- of banning terms that are perfectly legitimate in other contexts -- rises with every word you add to the stop list.) With a bottom up approach to terminology control, you get an accurate measure of which terms are being misused, and the frequency and nature of the misuse. This is an excellent basis for compiling a useful stop-list. 
        
        Also, because subject annotation can specify the type of a term (that is distinguish between `{Blue Jays}(baseball-team)` and `{Blue Jays}(bird)` you can make your stop list type specific: banning works used in one sense but not another. This can greatly reduce false hits, which is important because people tend to abandon the use of conformance tools if they produce very many false hits, as we can see from the very infrequent use of the grammar checkers in word processors. 

        As your taxonomy develops from the bottom up, it may be tempting to start to impose it from the top down, but this is probably not a good idea (though there may be exceptional circumstances). Having to comply with an top-down taxonomy while writing divides the writer's attention and compromises functional lucidity, both of which are bad for content quality and the author's efficiency. Continuing to rely on feedback to catch errors after initial authoring allows writers to stay in the moment as they write. If the feedback is given to the authors and they fix the errors themselves, they will learn the correct terminology and tagging over time and will make fewer mistakes. However, the door will remain open for new terms to be allowed into the content as appropriate, and to bubble up to the taxonomy, rather than banning them from the content until the taxonomy can be updated from the top down. 
        
        This approach does not make all vocabulary constraint problems go away, but it does have a number of advantages. 
        
        * It turns an up-front taxonomy development effort into an permanent part of the content development process. This not only reduces the spike in effort, it means that the taxonomy is based on real experience writing real content and that it is continually maintained as subjects and business objectives change. Indeed, if you are already annotating your content  to support any of the other structured writing algorithms, you essentially get taxonomy development, maintenance, and control almost for free. (Someone does have to review the reports, make the edits, and add to the canonical taxonomy, of course.)
        
        * It improves {functional lucidity} by not forcing authors to refer to the taxonomy while writing. If you are already annotating for other reasons, you are imposing no additional burden on authors at all. 
        
        * By improving the consistency of the annotations, it makes all the other algorithms that rely on them more reliable as well. (This is one of the greatest virtues of the subject domain. Subject domain markup can serve multiple algorithms, meaning you get the benefits of multiple algorithms with less cost. 
        
    section: Local and global names

        A big part of functional lucidity is the ability to call things by the names that are familiar to you. We noted in the discussion of the {relevance algorithm} ([#chapter.relevance]) that the use of clearly labeled unambiguous identifiers can help algorithms determine the relevance of a piece of content. The problem is, not everything has a globally unambiguous identifier (such as a company stock ticker) and even for things that do, the author may know know what those identifies are off the top of their heads (for instance, many authors may know the names of companies like Apple, Google, and Microsoft but not know their full ticker symbol). Forcing them to look them up every time you want to to unambiguously identify a company will add a lot of overhead to the authoring process, and it will also make the markup of the information more complex, again impacting the efficiency of authoring. 

        Fortunately, we don't have to unambiguously identify everything we write about at a global scale. We only have to ambiguously identify it within the context of the content we are writing. There is actually a universal truth about language in this. Very few words have only one meaning or identify only one thing. "Sun" is a big ball of burning gas and a computer company that was bought by Oracle. We distinguish these meaning by context, and we can distinguish content identifiers by context in the same way. 
        
        In fact, doing the identification in context is actually more accurate, since there is less possibility of accidentally introducing a confusion with a usage you are not aware of. The more highly contextualized an identifier is, the less ambiguous it is (as long as you specify both the context and the identifier you are looking for).   
        
        
        Rather than forcing the author to add this additional namespace information when they write (the equivalent of calling everyone by their full name and address around the dinner table), we can have the synthesis algorithm add it based on what it knows about the source of the content. This simplifies the author's task, which means they are more likely to provide the markup we want. (It is also another example of factoring out invariants, since we know that all method names in this particular piece of content will belong to the same API.)
            
        The namespace of a person's name is implicit in where they live. It is not that people lack a last name or address when they are at home. It is that we don't use those those additional identifiers because they are cumbersome and the first name is sufficiently unique to establish who we are talking about. Similarly, content created within a domain has its namespaces implicit within the domain in which it is created. 
        
        When we pull content from multiple domains, however, as we might do for purposed of {content reuse} or simply because we have divided authoring of a large content set into multiple domains to make it easier to create and manage, we need to make the namespaces of all the local names we use explicit in the content as we pull it out of the local domain. This can be done by algorithm because the algorithm can know which domain each pieces of content is coming from. 
        
        
        
        Placing names into namespaces does not magically resolve all disagreements about what to call things in a wider information space. But it does allow for an {information architect} to choose an definitive mapping of names from each namespace into the enterprise namespace. The results may still be disputable, but at least they will be consistent. And if they are disputed, and a different mapping is accepted, only the mapping has to change to put the new system into effect. As long as each pieces of content is tagged correctly according to the rules of its local namespace, it does not have to change just because the rules of the enterprise namespace change. 

        Achieving agreement (and, what is really more important, functional lucidity) within a local domain is easier in some domains than others. The {media domain} is highly concrete, so there really is not much room for disagreement there. Styles, though, are often given names from the document domain, as they are really a step into the document domain, and this can lead to disagreements because of the more abstract nature of the document domain. 
