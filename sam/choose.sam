chapter: Choosing

    <<<(annotations.sam)
    
    index:: type, term
        task, choosing 
        
    The information creation and delivery process ends when a reader receives a clear idea which enables them to act successfully (either in your interest or in their own). Part of the complexity of that process lies with the reader themselves. Readers do not always have a clear idea of what information they are seeking, or even what goal they are trying to achieve. They often lack the vocabulary of the domain of discourse in which experts in the field communicate. They often have to learn a lot of background before they can form their question correctly or understand its answer fully. In many cases, their goals change as their understanding grows. It is an entirely proper goal for the writer to what to help the reader deal with their part of the complexity of information retrieval as much as they can, but there are limits to what they can know about the reader, their goals, and their background, and there are limits to how much it is possible for an outside party to influence the development of understanding in the reader. 

    In short, there is no Nurnberg Funnel, and there can never be one. Worse, the limit of what we can hope to do for the reader is fuzzy, and different for different readers. This makes it impossible to define perfection for an information delivery system. You cannot achieve perfection is the perfect state cannot be precisely defined.

    If the goal of structured writing is to help manage the complexity of the content system in such a way that no person or process in the system is ever faced with more complexity than they can handle, it is impossible either to define or achieve a perfect structured writing system, or, more broadly, a perfect content management system, or, more broadly still, a perfect content system. 
    
    Worse, it is very difficult to measure the amount of complexity the reader is dealing with. Not only is it different for each reader, it is hard to measure the difficulties even one reader is facing. Worse still, it is hard to measure the economic consequences of the amount of complexity that the reader is facing in dealing with the complexity of information seeking, and it is hard to estimate the economic benefits that can be reaped by lessening that complexity. 
    
    Culturally we have long accepted that research is hard work. A large part of standard education has been devoted to teaching people to do research, and it is the ability to do research that is considered the hallmark of an educated person. The Web has made finding information radically easier in a way that calls this emphasis on research as a foundational skill into question. Finding information is now trivially easy, though assessing its validity is highly problematic. In the past, the academic and publishing complex took on the role of filter, creating a large body of putatively reliable information in which it was very difficult to find information, but the information you found was putatively reliable, or at least its bonafides could be assessed in a more of less formal way. This has been replaced, or laid in parallel beside, a system in which finding information is easy but establishing bonafides is much more complex and depends on a whole different set of mechanisms. 

    Since we cannot remove all complexity from the reader, and since we cannot precisely define the minimum of complexity we can leave to the reader, and since, even if we could, we cannot effectively measure how much complexity the reader is actually dealing with, most content systems have never really seriously attempted to minimize the amount of complexity they dump on the reader.    
        
        
        
    How do you choose the right structured writing approach for your organization. Quite simply, you figure out how you want to partition and distribute complexity in your content system, and then you choose the languages, systems, and tools that allow you to implement that partitioning most efficiently. You may be able to do this will off the shelf tools, by customizing tools or frameworks, or by developing certain elements of your solution in house (or having a consultant do it for you). 
    
    Developing custom structures and algorithms and maintaining them is, of course, part of the complexity that tools always add to your content system, and so such development much be taken into account when you work out how to partition and distribute content complexity in your organization. But you should realize and accept that in some cases, without some degree of custom development it will be impossible to fully partition and direct complexity to people with the right skills, knowledge, and resources, without dropping any of that complexity and letting it fall through to your customers. 
        
    section: Complexity
    
        Unless you are writing directly on paper, chiseling into stone, or drawing letters by hand in a paint program, you are using structured writing techniques to create content. You are using tools that have been designed to partition and distribute some part of the complexity of content creation in some particular way, and you are observing a discipline, as set of constraints, imposed by that partitioning. The question is, is that partitioning ensuring that all the complexity in your content creation process is being handled by a person or process with the skills, resources, and time required, or is complexity going unhandled in your system and getting dumped on the reader?
        
        If not all of your complexity is being appropriately handled, if complexity is falling through to your readers in the form of quality problems, then the question becomes, how do you change your approach to structured writing to handle complexity better. It is important not to approach this problem piecemeal. Complexity cannot be destroyed, only redirected, and every tool you introduce introduces new complexity which much also be partitioned and redirected. Attacking one piece of complexity in isolation usually results in complexity being directed away from the area you attack, but if you don't think about where that complexity is going, and how it will be handled, you can easily end up with more unhandled complexity that you started with.
        
        The growth in unhandled complexity will often be interpreted (especially by the advocates of the system you have just installed) as a change management problem or a training problem. But while change management and training are both necessary any time you change how you do things, the most likely cause of problems after the installation of a new system is design problems, specifically a system design that redirects complexity away from itself without consideration for how it is going to be handled. 
        
        We should not assume either that the correct response to this problem is a new project to address each of those pieces of redirected complexity. A better solution will often be to rethink the approach to the original problem you attacked to see if a different partitioning and redistribution of complexity may produce less harmful side-effects -- if any complexity it directs away from itself, as opposed to handling internally, is calculated to be something that an external process and easily and reasonably handle. 

        Content creation, particularly content creation at scale and in a dynamic environment is not simple. Structured writing can deliver many benefits in quality and efficiency, but it cannot magically make the whole process simple. 

        In fact, structured writing systems don't make any of the complexity of content creation go away. What they do is move that complexity from one place to another. For instance, {separating content from formatting} moves the complexity of formatting text away from the author and places it on a formatting algorithm, and therefore on the person who writes and maintains the formatting algorithm. Using the subject-domain approach to {linking} moves the complexity of finding content to link to away from the author of the source content and places it partly on an algorithm, but also on the author of the content that is being linked to, since they must accurately index their content to indicate what subjects it covers. 

        The point of moving complexity from one place to another is to move each type of complexity to the person or process who is best placed to handle it. Of course, the machinery we use to move this complexity -- structures and algorithms -- also introduces its own complexity into the process. This means that structured writing is actually adding to the total amount of complexity in the system, which makes it all the more important to ensure that that complexity is moved to the right place. It is all too easy for the introduction of a structured writing system to make the content process less efficient and the resulting content of lower quality, simply because it does not move the complexity to the right place or introduces more complexity into critical functions than it removes from them.    
        
        For example, separating content from formatting requires that we factor out the formatting and replace it with structures from which the formatting can be derived. But if we turn to a complex document domain markup language like {DocBook} for this, we will often find that we have made the author's task more complex than it was before. 
        
        The very fact the structured writing has the ability to move the complexity around the system, imposing it on one role at the expense of another, makes the task of choosing a structured writing approach all the more complex. For instance, there has been a long history of IT departments choosing content management system on the basis that they were easy for the IT department to install and administer, only to have them be deeply unpopular with users because they pushed complexity out to authors and other users. On the other hand, some groups of authors want to write in uber-simple formats like Markdown, despite the difficulties that its limited structure and capabilities create for an overall publishing process. 
        
        Inconsistency, duplication, delay, error, and failure are escape valves for the complexity the no part of the organization is willing to accept. Traditionally, a style guide has become the dumping ground for complexity. Every new rule and requirement gets written into an ever growing style guide. 
        
        Change management is another scapegoat for complexity that no one will own. It is very common to blame the failure of structured writing systems on unwillingness of writers to change. The cure, the backers of the system assure us, is more training and more generalization of the change. But what is usually going on in these cases is that the system design has dumped new and unmanageable complexity on whatever group is rebelling -- almost always the writers, either the full time writers or the occasional contributors. 
        
        Here is the inescapable fact: complexity is irreducible. It can be moved, not destroyed. When you simplify one processes, therefore, you have added complexity somewhere else in the system. It is all too easy to get caught up in the gains you expect from removing complexity from A and forget to pay attention to what the effect will be of the complexity you have added to B. The right way to design a structured writing system, therefore, is not to focus solely on where you can create simplicity, be were you can move complexity to, and how well, and how reliably, that complexity can be handled in that place. 
        
        Complexity that is not handled in the organization is dumped onto the customer. This can take the form of sub-optimal information architectures, such as vast hierarchical organizational schemes that make no sense to the reader, or a lack of linking to allow the reader to easily find their way from one subject to another. It can also manifest itself in poor information design, in topics that do not adequately define their content, leading to users not being able to identify them as the the place to find the information they need. It can also manifest itself in simpler things like lack of information, information expressed in the wrong way, or a general lack of consistency that makes navigation and comprehension difficult. 
        
        It is natural for designers and implementors to distribute complexity away from themselves. 
        
        There are knock-on effects to complexity as well. If authoring is complex, collaboration becomes complex. So does conformance. 
        
        The right mix and distribution of complexity and simplicity is likely to be different from one organization to another, even if all parties have a good understanding of where complexity lies and how it can be distributed or minimized. It is beyond the scope of this book to walk you through this decision making process. I hope that what this book has contributed to that process is an understanding of what structured writing is capable of bringing to an organization, and a tool-neutral language for talking about it. 
    
    section: Scale
    
        The question of whether it is worthwhile to transfer complexity from one part of a process to another often depends on scale. Transferring complexity from a human to a an algorithm, for instance, creates a complex task of writing, testing, and maintaining the algorithm. But algorithms don't cost any more to write, test, and maintain based on the amount of content you process with them. You have to pay a human every time they do the task by hand. You only have to pay them once to write and algorithm to do it automatically. 
    
        All of the algorithms we have discussed so far can be carried out either by humans or by algorithms. Algorithms are much faster than humans, but algorithms have to be designed, written, and maintained, and they don't have the same capacity as humans to adapt on the fly when conditions warrant it. They also require much more precisely structured inputs than humans, meaning that they require more work from human writers, at least for original content creation. (They may save writers work in all sorts of other ways, as we have seen.)

        How do you determine when it is worth the investment to design structures and algorithms and have your writers write structured documents, rather than having them just execute all the algorithms themselves. Issues of consistency and quality are important considerations here. There are also some information designs that are very costly to develop without the aid of algorithms. However, one of the key factors in making this decision is scale. 
        
        The issue of scale applies to many systems and many activities. A home kitchen does not work like a restaurant kitchen. The restaurant kitchen has multiple work stations as divides the work up among multiple cooks. They prepare ingredients in advance so that when an order comes in, they can prepare it very quickly. All the equipment, planning, and preparation costs money, but it pays off when you have to process a lot of food orders during a busy dinner shift. By contrast, a cook in a home kitchen starts with basic ingredients and does all the steps themselves to prepare a single meal for themselves or a small group. 
        
        The restaurant could not possibly keep up with orders if it worked the way a home kitchen worked. That method of preparation does not scale up to the throughput of a restaurant. Equally, though, the approach of a restaurant kitchen would not work for a home kitchen. The overhead of all the stations, the planning, and the advanced preparation would not be economical for preparing a single small meal. The approaches of a large-scale operation do not scale down to a small-scale operation anymore than the approaches of a small-scale operation scale up for a large organization. 
        
        The same is true of writing. 

    
    section: Tool driven vs. structure driven decision making
    
        I do want to say one thing about the decision making process, though. It is important to decide up front if you are going to have a tool-driven decision process or a process driven by algorithms you want to use. 
        
        Of course, your decision making process should ultimately be driven by business requirements. But even a process driven by business requirements tend to be heavily influences by what we think the available tools are capable of. We don't tend to write requirements for systems unless we have an idea that there are systems out there that are capable of meeting those requirements. 

        Over the years I have seen many requirements documents for proposed structured writing systems that essentially said that the proposed system must work exactly like Microsoft Word. This is not surprising when the people writing the requirements have used nothing but Word to create content for years. The tools you know shape how you work and what you think of as possible. As Henry Ford is supposed to have said of the Model T, "If I asked customers what they wanted, they would have said faster horses." Even when we are dissatisfied with our current tools, we tend to want the same basic tool only more so. Faster horses. This is why so many structured writing tool vendors literally advertise that their editor looks and feels "just like Microsoft Word". (Not to mention those vendors who create tools that modify Word itself.)

        But Microsoft Word is a tools that sits on the boundary between the Media and Document domains. Using Word itself, or something that looks like Word, is usually an attempt to move its use slightly more into the document domain, but as we have seen, the WYSIWYG authoring interface invites a slide back into the media domain by hiding the structured that is supposed to be created and showing only the formatting that is supposed to have been factored out in adopting the document domain. 

        It is little to be wondered then that the structured writing tools that have been popular in the market to date have been predominantly document domain tools, and have tended, like DocBook, to be very loosely constrained. (It is much easier to write an XML document in a WYSIWYG editor if the underlying structures are minimally constrained, since it lets you insert whatever bit of formatting you want anywhere you want. 

        Even with tools like DITA, which, while it is still fundamentally a document domain system, is more constrained, and capable of being constrained further, tend to be used in its generic out of the box form and with a Word-like WYSIWYG interface.  

        Thus even when a decision-making process is based on business requirements rather that specific tools, it is often tacitly driven by existing tool sets and ways of doing things, because those existing tools and processes shape our view of what the business requirements actually are. We don't ask for a way to get from DesMoins to Albuquerque, we ask for a faster horse that eats few oats.   

        By laying out the structured writing algorithms as algorithms rather than as features of a tool, and by showing the different ways in which those algorithms can work based on content in the different structured writing domains, I hope I have provided you with a way to break free of the dead hand of current tools and processes in thinking about your structured authoring decision. 

        If so, this will allow you to have a decision making process based on algorithms and a knowledge of how the algorithms and the structure that support them can work together to address your business problems, while putting the complexity in the right place for your organization. 

        If you can come up with a set of requirements that say that you want to execute a give set of algorithms with a given level of reliability and to distribute the complexity of the system to an appropriate set of roles, then you will have a much more neutral basis on which to approach vendors or consultants.

        Every tool in the market place, from end to end systems, to individual tools like XML editors, to frameworks and tools kits, represents an encapsulation of algorithms and the structures that support those algorithms. Not only do they represent a view about which algorithms are most important, but also a view about how they should be implemented -- which ultimately means how and to whom the complexity attendant of the algorithm should be distributed. Every tool, therefore, can be represented as a set of algorithms and structures, just like your set of requirements. 
        
        The decision making process then comes down to seeing if the best match between capabilities and requirements is to be found in a single end to end system, in a combination of separate tools, in modifying or building upon and existing framework, or in building some or all of the components of your system from scratch. When talking to any tool vendor, you can ask them to demonstrate how their tool supports each algorithms you care about, how reliable that implementation is, and where it distributes the complexity. 
        
        While individual circumstances can vary greatly, there are general patterns in how the use of different domains to implement algorithms distribute complexity within a system. Media domain systems are simple for small things, but provide no help for any kind of management or single sourcing algorithm, distributing all the complexity of those functions to the author and content manager. (Which results in complex content management systems.) They simplify ad hoc formatting of individual items but distribute all the complexity of conformance and consistency to authors. In short, they are simple in themselves but do nothing to alleviate the complexity of any other part of the publishing process. 

        The document domain distributes formatting complexity away from authors to the creators of publishing routines. However, a different kind of complexity can easily take its place if the number of document domain structures that people have to remember and use grows in order to support complex output requirements. If single sourcing is required, the document domain again distributes some of the complexity away from authors to the creators of publishing algorithm, with the limits we have noted in regard to {differential single sourcing}. By itself, the document domain, like the media domain, does little to alleviate other parts of the publishing process. 
        
        The management domain, in concert with the document domain, enables a range of content management features that would otherwise be either tedious or impossible. Content reuse is probably the prime example of this. Content reuse in pure media-domain or pure document-domain content is generally so complex that it would not be attempted without the introduction of management domain structures. However, management domain structures distribute a huge amount of complexity to authors, effectively shutting out contributors without specialized training and tools. 
        
        The subject domain distributes a huge amount of complexity away from authors, and, by making so many of the structured writing algorithms more reliable, it also distributes a lot of complexity and effort away from content mangers. However it distributes that complexity to the information architects and tools people who design the subject domain languages and write the algorithms that process them.  
        
        Complexity is by no mean the same thing as effort. Structured writing systems that are well designed to support appropriate algorithms can reduce overall effort considerably, which significantly improving quality. But where they place complexity matters. Even if a task requires less effort, adding complexity to it changes how the person assigned to that task works, and how they need to be qualified and trained. It is important to appreciate how the distribution of complexity and effort in the system you choose affects the dynamics and composition of your team. 
        
        As is no doubt apparent to you by this point, I am a strong believer in distributing the complexity of a structured writing system away from writers. The reason for this is simple. When structured writing systems distribute complexity toward writers, they don't merely add a new and complex task that must be learned, they impose that complexity directly on the activity of writing itself. It is not really possible for the writer to segment the writing process from the process of creating structure. They are too bound up with each other. The whole point of structured writing is that they should be bound up with each other. Yet writing is an activity requiring the whole of ones attention. Any unavoidable division of that attention directly detracts from the quality of writing. 
        
        Any avoidable complexity should be purged from the writing process itself. Not all markup represents an addition of complexity, however. Effective subject domain markup can work to direct the writers attention into the appropriate channels, distributing design complexity away from the writing task and leaving more attention available for the task of writing itself. (This is the basis on which we use forms for all the things we use forms for.)

        Unfortunately, structured writing systems are often designed with other priorities in mind. In particular, they are often designed with content management priorities in mind. Structures a designed to support content management algorithms with little attention paid to how those structures distribute complexity towards authors. 

        In the end, the question of where complexity is distributed in your system is at least as important, if not more so, than the question of how much effort is avoided. The wrong distribution of complexity can not only undermine quality, it can also undermine the attempt to reduce effort. Complexity in the wrong place not only undermines the productivity of those saddled with it, it also undermines the reliability of every other algorithm, thus undermining the attempt to reduce effort and cost in those algorithms.     

        The economics of this decision are clearly complex. You may decide that the cost of creating and maintaining the most appropriate algorithms and structures in not worth the cost or quality improvements they promise. But hopefully that decision can be made with a full appreciation of the benefits that those algorithms are capable of delivering. But whatever you decide, make sure you understand how complexity is being distributed in the systems that you implement, and very conscious of the ability of those you are distributing it to to handle it, and the effect it will have on their productivity and the reliability of their work.     
        
    section: Indivisible complexity    
                
        There are complex tasks which by their very nature have to be done by one mind. But there are also many tasks which have to be dome by one mind only because to partition them requires a transfer of information between one mind and another that we lack the means to perform. 
        
        
        Because attention is a limited resource, there is value in partitioning and distributing tasks even when you perform all of the tasks yourself. Partitioning the tasks allows you to give your full attention to each individual task and thus perform better than you could if your tried to divide your attention between them while performing both at once. 
        
        
        Additionally, partitioning complexity allows you to distribute parts of it to people with other skills sets who do not have the interest, attention, or skills to do the whole thing, but who can do parts of it adequately, or even superbly.
        
       
        In some cases, we reduce the complexity that falls on each contributor by gathering a particular complex operation from the many and distributing it to one uniquely qualified or equipped person. (Example, formatting.) In other cases, we do so by taking a complex operation currently being performed by a single person and distribute it out to many contributors. (Example, maintaining a Wiki.)
        
        
        The problem is, content, in its natural state, is not in a format that allows us to partition the complexity of writing from the complexity of formatting. If we are going to redistribute some aspects of complexity from writers to algorithms, we have to get the content into a form that algorithms can read. This, of course, introduces new complexity into the writing process. 
        
    
    section: Granularity
    
        There can be a conflict between ease of authoring and ease of {content management}. Content management may want to manage content down to a fine level of granularity, especially for purposes of content reuse. This content management algorithm may be best served by managing fairly small chunks of content -- semantic units rather than narrative units. But for the writer, something less than a narrative unit can be difficult. It can be difficult for the author to get a sense of how the semantic block they are writing will meet the reader's needs when they don't see the narrative block it will fit into. It is hard to create parts rather than wholes unless the parts are really well defined. A writer might carry the whole of an essay in their head, for instance, and be able to structure it well on that basis. But if they are making only parts and cannot see the wholes that will be created, it is hard to correctly structure a part without very clear and explicit guidance.  

    section: Extra
    
        Moving your content to the subject domain, in other words, is about doing the simplest thing that works to achieve a given outcome of a given quality. Sometimes {MarkDown} is the simplest thing that works, but there are lots of quality, management, and production goals that are hard to meet with {MarkDown}. Going to elaborate {document domain} languages is often the next step people take in pursuit of those goals, but these are not simple. In many cases, the {subject domain} may be the simplest thing that works for these larger goals. But it make it so, you need to make sure that your subject domain languages are as simple as they can be to get the job done. 



    section: bits

        Communication overhead is a source of complexity. There is a huge benefit to developing interfaces that reduce the need for communication between collaborators. 
        
        If the writer is thinking visually then giving them the tools to execute their visual removes the need for communicating with a designer and a layout artist. This was the DTP approach to this partitioning problem. The document domain approach is to try to get the writer to think in terms of abstract document structures rather than formatting. Again, this remove the need for communication with the designer and layout artist.
        
        Many tools are designed to attack one form of complexity or another  and are careless about where the complexity they create, or the existing complexity that they create, falls. Many DTP and structured writing tools, in particular, sought to solve publishing or content management problems by dumping complexity on writers, effectively shutting out many people in the organization out of the content system, or forcing them to set up separate rival content systems that they could manage. And since even the trained authors were often having more complexity dumped on them than their attention could manage, a lot of the complexity got dumped on the reader as well. 
        
        Quality, change management, linking, etc can all be great sources of efficiency without recourse to the complexity and quality problems that plague many reuse schemes. Repeatability may save you far more money than reuse. It is reuse of design. Besides, reuse without repeatability is a recipe for poor quality and content management breakdown. 
        
        Principle\: partition and direct toward expertise. If the expertise is distributed, direct it outwards. If it is centralized, direct it inwards. Bottom up taxonomy, for instance, partitions language choices towards writers while allowing discovery and conformance to flow back to taxonomists. 
        
        There is a limit to how much partitioning you can do because content is irregular data. Some is more regular than others, but all are related to each other, so even the most regular content has irregular relationships. The trick is to impose constraints two make content manageable and repeatable without being false to the fundamental nature of the data. 

        Simplicity does not result from destroying complexity but from partitioning and redistributing complexity away from a person or process. But certain functions, such as reuse, make this difficult to do. They can have dangerous knock-on consequences.
        
    section: Reuse
    
        It is easy to talk about content reuse as if it were a single thing. But not only are there many techniques, there are also many different use cases. Some advocates will want to look at reuse as a single bulk number: percentage of reusable content, percentage or reuse achieved. But reuse looks different, and has different advantages and disadvantages in different use cases. Also, different techniques fit different use cases. 

        section: Produce variants
        
            Many companies produce multiple products based on the same technology to serve different needs and different price points. Much of the information is common between products but different product versions have different features added. Most car models, for instance, come in half a dozen variant from the cheaper base model to the much more expensive full loaded variant.
            
            This is a very natural and well constrained type of reuse. The content is address to the same audience for the same purpose. It is reused because the technology that it describes is reused, and its reuse has a direct correspondence to the reused technology. Subject domain techniques work extremely well for this type of reuse because it is based on reuse of the subject matter. This means that with the right techniques, it is highly manageable and reliable. Again, with sound technique you are not likely to get into trouble or to compromise quality with this kind of reuse. 
            
            Do be aware, however, that while putting out a variation of the manual for each product makes perfect sense on paper, it can create navigation and search problems on the web, were users are quite likely to land on the manual for the wrong model when they search for a common feature. Providing for easy identification and navigation between models is essential. Alternatively, publishing only one copy of a feature description, marked with the models it applies to, solves a lot of search and navigation problems. If you choose this approach make sure that you are creating structures that support this form of {differential single sourcing}. Most management domain reuse techniques will not support this. 
            
            Be particularly careful not to get into a situation were you are measuring yourself by the percentage of your content that is reuse. For one thing it can discourage you from making good design choices like publishing a feature description one marked with the models it applies to. For another, with product variants you reuse number is driven by the number of variants the company decides to produce. You are not really doing better every time they introduce a new variant, or worse every time they trim the product line. 
            
            Product variation can be supported with an assemble out of piece approach, but filtering also works and may require a less radical rethink of your tools and writing habits. 
            
        section: Audience variants
        
            Many organizations need to address different audiences on the same subject matter for different purposes. Some reuse advocates will use the example of reusing a product description in a technical manual, a brochure, a catalog, and a white paper. 
            
        section: Boilerplate
        
        section: Textual coincidences
        
    section: DITA
    
        DITA is such a dominant feature of the structured writing landscape at the moment that it is necessary to say something about it. 
        
        DITA, on the other hand is based on a rhetorical theory about what the building blocks of a document are and how they go together. That theory may be very foreign to the way you write and almost certainly a departure from the tools you have been using. 

        People rushing to DITA for its reuse capabilities often seem to be unaware that they are also buying into a rhetorical theory. Paligo essentially is providing similar reuse capability sans the rhetorical theory and the constraints that go with it.
        
        In evaluating DITA it is important to remember that you are getting two things in one box: a set management domain structures optimized for every type of document domain content reuse, and the generic implementation of a particular rhetorical theory. You can get various bundles of reuse capability from a number of other tools without the rhetorical theory in tow. Logically, therefore, the reason to choose DITA would be that your principle needs are met by the combination of its two components: its reuse capability and its rhetorical theory. You can get the rhetorical theory alone from information mapping and the reuse capability alone from several other tools. 
        
        DITA's specialization feature is not merely a technical feature, it is an extension of its rhetorical theory. It is not a tool that is necessarily suitable for other approaches to rhetorical modeling even if it is technically capable of achieving them. It is optimized for the block and map model of document construction and it a clumsy instrument for rhetorical modeling at the document level. Since it should be obvious to the reader by now that I believe strongly in the power of rhetorical modeling at the document level -- it is one of the central pillars of the Every Page is Page One information design approach -- I am not a fan of DITA. Everything I say about DITA in this book should be understood through the lens.
        
    section: The importance of the rhetorical model
    
        If the aim of structured writing is to partition and redirect complexity in the content system without letting any of that complexity leak out of the process and fall down to the reader, then the correctness and consistency of rhetorical models is a core concern. Poor rhetoric means poor content, and poor content means that the complexity of achieving consistent and correct rhetoric has been dropped somewhere in the content system. 

        A content system relies for its effectiveness on the ability of its principle authors and occasional contributors to maintain a consistent rhetorical standard. The three tools that it has available to do this are:

        * Minimize intrusions into the attention of writers while they are writing. Any attention given to other matters while writing is attention taken away from rhetoric. 
        
        * Guide writers to help them provide the correct rhetoric. In other words, reuse the rhetorical design work that you have done, and that you have tested and refined with readers.

        * Partition out the rhetorical aspects of a composition by collecting facts in a subject domain structure which can then be transformed into the appropriate rhetorical form by algorithms. 

        Content inherently varies in how structured it is, so the more structured of these techniques only work for some part of your content set. Your need all three of these techniques to provide the most comprehensive rhetorical support across your content system.          
    
    section: The transition to hypertext
    
        Many structured writing decisions are driven by short term desired to solve immediate problems like content reuse. But the world in in the middle of a transition from paper to hypertext. Almost every organization currently has to produce both, but most are not doing both well. And while specific requirements for paper may always exist, most information requirements in the future are likely to be met through hypertext. Our transition to a networked world is by no means complete. But the ongoing proliferation of networks and devices, the capacity to deliver content hands free through heads-up displays, the ability to build context sensitive devices that can bring up content relevant to what the user is looking at, and ongoing improvement in security, and a gradual shift in habits and regulations are moving us more and more into the hypertext world. 
        
        Reuse is a technique largely of the paper world. Single sourcing, and differential single sourcing in particular, belong largely to the transition period. Linking and the techniques of a bottom-up information architecture belong to the hypertext world. If you choose your tools and your structures primarily on the basis of the needs of the paper world or the transition period, you are likely to find yourself going through a similar upheaval in just a few years. As I have attempted to show, however, the subject domain supports all algorithms equally. Using your subject domain structures to drive reuse and differential single sourcing today and to drive linking and bottom-up information architecture tomorrow does not require a great upheval of tools and structures. 
