chapter:(#chapter.design) System design

    <<<(annotations.sam)
    
    index:: type, term
        task, system design 
        
   
	Unless you are writing directly on paper, chiseling into stone, or drawing letters by hand in a paint program, you are using structured writing techniques to create content. You are using tools that have been designed to partition and distribute some part of the complexity of content creation in some particular way, and you are observing a discipline, a set of constraints, imposed by that partitioning. It is not a matter of whether your system is structured or not, but how and for what purpose it is structured, and whether that purpose is being achieved.
    
    The question you need to address, therefore, is whether your current partitioning is ensuring that all the complexity in your content creation process is being handled by a person or process with the skills, resources, and time required, or is complexity going unhandled in your system and getting dumped on the reader?
	
	If not all of your complexity is being appropriately handled, if complexity is falling through to your readers in the form of quality problems, then the question becomes, how do you change your approach to structured writing to handle complexity better? 
    
    It is important not to approach this problem piecemeal. Complexity cannot be destroyed, only redirected, and every tool you adopt introduces new complexity which must also be partitioned and redirected appropriately. Attacking one piece of complexity in isolation usually results in complexity being directed away from the area you attack, but that complexity goes somewhere, and if you don't think about where it goes, and how it will be handled there, you can easily end up with more unhandled complexity in your system than your started with.
	
	The very fact that structured writing has the ability to move the complexity around the system, imposing it on one role at the expense of another, can lead to piecemeal rather than comprehensive solutions as different groups or different interests try to attack their part of the problem. For instance, there has been a long history of IT departments choosing content management systems on the basis that they were easy for the IT department to install and administer, only to have them be deeply unpopular with users because they pushed complexity out to writers and other users. On the other hand, some groups of writers want to write in uber-simple formats like Markdown, despite the difficulties that its limited structure and capabilities create for an overall publishing process. Everyone tries to simplify their own lives, heedless of the expense to others. 	
	
	Here is the inescapable fact: complexity is irreducible. It can be moved but not destroyed. The only way to remove complexity from a process is to stop doing something. Actually, the first question you should ask in any assessment of your process, is what things you are doing that you should simply stop doing. What is not creating any value? But let's assume that you have done that. Now that you have stopped doing things that you did not need to do, simplifying one function, will always move complexity somewhere else in the system. Moving complexity somewhere else in the system is fine if it is moved somewhere where it can be handled by an appropriate person or algorithm. The point is not the eliminate complexity, but to make sure it is handled correctly. The enemy is not complexity, it is unhandled complexity.

    But it is all too easy to get caught up in the gains you expect from removing complexity from A and forget to pay attention to the effect of the complexity you have added to B. The right way to design a structured writing system, therefore, is not to focus on where you can create simplicity, but to were you can move complexity so that it can be handled efficiently and reliably. 

	How do you design a content system that ensures that all complexity (or as much of it as is reasonably possible) is handled by a person or process that has the right skills and resources to handle it? You start by understanding the sources of complexity in your content system. 
	
    section: Complexity audit 
	
        Where is the complexity in your content system? It is different for every organization. While the basic functions of content development are the same for everyone, the amount of complexity they generate differs greatly depending on your needs and circumstance. If you translate into 35 languages, translation generates more complexity in your content system than in the system of an organization that translates to one language, or to none. If you have a complex product line in an industry with a complex vocabulary, terminology control creates far more complexity in your system than for someone with a simple consumer product. In order to know where you need to partition and redirect complexity, you need to identify where the biggest sources of complexity are in your content system. Here are some places to look:
        
		section: Current pain points 
        
			You embarked on this journey because some part of your current process is painful. Either there are quality problems of production difficulties. The trick is to figure out what the complexity is that is overwhelming that part of the process or is falling through in the form of quality problems. The underlying complexity can be hard to see, even when it is causing pain. Familiarity with your current process may make it difficult to see that things could be different. Seeing that there is another possible way to do things often provides the insight into the hidden complexity of the current process. Hopefully the chapters on the structured writing algorithms and the different ways that they are handled will help you see your current processes in a different light, and therefore help you to identify the complexity in them, and assess whether it is being adequately handled. 
            
            In order to find the true source of the complexity, however, you may have to work your way upstream from the place where the problem is manifest. Ultimately, all unhandled complexity finds its way to the reader, so quality problems are usually the place that complexity will show up if it is not being handled properly. The other place that you may find the impact of complexity is in processes that seem too expensive or in staff that seem overburdened or burnt out. 
            
            Lets say that you observe a quality problem: readers are getting lost in the content. They find an initial page, but they need more information on the concepts it mentions. But when they search, they end up back at the same page. What is the root cause of this problem? It could be one of (at least) three things:
            
            1. The content they are finding is incomplete. The information should be there, but it's not. 
            
            2. The content they need is not in the content set anywhere because no one realized it was needed. 
            
            3. The content they need is in the content set but they can't get to it because there is no link to it in the page they are on. 
            
            If you determine that the first issue is to blame, then the complexity that is going unhandled is that of determining the reader's need on a particular subject and making sure that it is always present in topics on that subject. You have a {repeatability} problem.
            
            If the second issue is the problem, then it is the complexity of content planning that is not being handled. You may need to improve your {audit} capability. 
            
            If the third issue is the problem, then it is the complexity of managing and expressing the complex relationships between subject that is not being handled. Perhaps the authors are not aware of the relationships, in which case you may have a rhetorical structure problem. Or perhaps they are aware but the cost of creating and maintaining links is to high, so they are skimping on linking. In this case you may need a different approach to {linking}. 
            
            Be careful when you do this analysis not to fall into two common pitfalls: 
            
            * Don't stop your analysis too soon, and don't be blinded by the limits of your current process. Trace the problem back to the actual source of complexity, independent of any tool or process considerations.
            * Don't stop with the first big chunk of complexity you find and rush off to buy a tool to address it. You need to identify all the source of complexity in your system and come up with a comprehensive plan to manage all of them, otherwise you will just move complexity around, while adding tool complexity to the mix. You can easily end up worse off that you were before. Even your biggest hot-button issue may demand a very different solution once you have figured out the total complexity picture in your system and the overall best way to partition and direct that complexity.
			
	    section: Unrealized possibilities
        
            Chances are that there are things you don't do today because they are not possible with your current tools, or any of the tools you are used to. It is easy to overlook these unrealized possibilities altogether because everyone else is dealing with the same tool limitations, meaning you may not see these possibilities realized elsewhere. But don't let the designs you can conceive be limited by the designs your current tools can execute. Those limitation just point to a design complexity that you don't currently have the means to handle. 

            A comprehensive bottom-up linking strategy is a good example of unrealized possibilities. If you are working int he document domain, and using document domain or management domain linking techniques, the then creation, management, and updating of links is expensive and complex to execute. In reaction to that complexity, you minimize the links you create, or create them in sub-optimal places. That complexity manifests itself in unrealized potential. 

            But if you switch to subject-domain linking based on subject annotations, you partition and distribute the complexity of linking away from authors and towards an algorithm, radically reducing the barriers to a comprehensive linking strategy and allowing the possibility of that design to be realized. (And note that you don't have to go to a full {subject domain} system to use this approach to linking. It works just as well with subject annotation in otherwise document domain information types.)            
        
        section: What's working now
        
			Points where your current process is working. You are currently partitioning and directing this complexity well, but your still need to inventory it to make sure it is still handled well in your new process. (And don't assume that because you are satisfied with this part of your current process that it is actually working optimally.)
            
            For instance, if you are generating reference information from source code using a tool like DoxyGen, that is a complex task that is currently being handled by an algorithm. Make sure you add that to your inventory so that when you design your new system you don't end up with that complexity no longer being handled properly. 
            
            At the same time, look around the edges of your current success stories to see if there is any unrealized potential there. Suppose your generated reference is being published through a different tool chain so it looks different and is not linked from or too the rest of your content. The is integration complexity that is going unhandled and falling through to the reader in the form of less usable and less pleasing content. 

            You may end up completely changing the way you handle the complexity you are successfully handling now in order to better distribute complexity across the whole system. But don't loose sight of the complexity just because your are currently handling it well.             
			
		section: Complexity that impacts others
        
			Points where the impact of complexity is falling on someone else. These are the pains you are not feeling because someone else is suffering the consequences. Often this is the reader, but it could be others in your organization -- support, sales, field engineering, etc. 
            
            Look for hidden complexity. What is not being handled? Where have you surrendered to the limits of your current tools? Do a regression analysis on you known content problems. They all arise from unhandled complexity? Where did that complexity arise and why was it not handled? 

			
		section: Culturally ingrained limitations
        
			Points were the limits of handling complexity are culturally ingrained. This is the area in which breakthrough innovations happen. It is where you find the product or service that no one knows they want until they see it, and everyone knows they want as soon as they do. This is your chance to wow. 
        
        section: Process overheads 
			Communication overhead is a huge source of complexity. DTP was revolutionary in its day mainly for eliminating communication overhead between writers, designers, and typesetters. There is a huge benefit to developing interfaces that reduce the need for communication between collaborators. If the writer is thinking visually then giving them the tools to execute their visual removes the need for communicating with a designer and a layout artist. 
			
			This was the DTP approach to this partitioning problem. The document domain approach is to try to get the writer to think in terms of abstract document structures rather than formatting. Again, this remove the need for communication with the designer and layout artist. When you design your new process, make sure you are not introducing new sources of communication overhead into your system. 
        
        section: The advent of hypertext      
        
            The world in in the middle of a transition from paper to hypertext. Almost every organization currently has to produce both, but most are not doing both well. And while specific requirements for paper may always exist, most information requirements in the future are likely to be met through hypertext. Our transition to a networked world is by no means complete. But the ongoing proliferation of networks and devices, the capacity to deliver content hands free through heads-up displays, the ability to build context sensitive devices that can bring up content relevant to what the user is looking at, and ongoing improvement in security, and a gradual shift in habits and regulations are moving us more and more into the hypertext world. 
            
            Reuse is a technique largely of the paper world. Single sourcing, and differential single sourcing in particular, belong largely to the transition period. Linking and the techniques of a bottom-up information architecture belong to the hypertext world. If you choose your tools and your structures primarily on the basis of the needs of the paper world or the transition period, you are likely to find yourself going through a similar upheaval in just a few years. As I have attempted to show, however, the subject domain supports all algorithms equally. Using your subject domain structures to drive reuse and differential single sourcing today and to drive linking and bottom-up information architecture tomorrow does not require a great upheaval of tools and structures. 

    section: Map complexity to the people and processes that handle it

        Principle\: partition and direct toward expertise. If the expertise is distributed, direct it outwards. If it is centralized, direct it inwards. Bottom up taxonomy, for instance, partitions language choices towards writers while allowing discovery and conformance to flow back to taxonomists. 
        
        
        
        Complexity is by no means the same thing as effort. Structured writing systems that are well designed to support appropriate algorithms can reduce overall effort considerably, which significantly improving quality. But where they place complexity matters. Even if a task requires less effort, adding complexity to it changes how the person assigned to that task works, and how they need to be qualified and trained. It is important to appreciate how the distribution of complexity and effort in the system you choose affects the dynamics and composition of your team. 
        
        In the end, the question of where complexity is distributed in your system is at least as important, if not more so, than the question of how much effort is avoided. The wrong distribution of complexity can not only undermine quality, it can also undermine the attempt to reduce effort. Complexity in the wrong place not only undermines the productivity of those saddled with it, it also undermines the reliability of every other algorithm, thus undermining the attempt to reduce effort and cost in those algorithms.     

        
        As is no doubt apparent to you by this point, I am a strong believer in distributing the complexity of a structured writing system away from writers. The reason for this is simple. When structured writing systems distribute complexity toward writers, they don't merely add a new and complex task that must be learned, they impose that complexity directly on the activity of writing itself. 
        
        It is not really possible for the writer to segment the writing process from the process of creating structure. They are too bound up with each other. The whole point of structured writing is that they should be bound up with each other. Yet writing is an activity requiring the whole of ones attention. Any avoidable division of that attention directly detracts from the quality of writing and limits the number of contributors. 
        
     




        

        section: Indivisible complexity    
                    
            There are complex tasks which by their very nature have to be done by one mind. But there are also many tasks which have to be dome by one mind only because to partition them requires a transfer of information between one mind and another that we lack the means to perform. 
            
            
            Because attention is a limited resource, there is value in partitioning and distributing tasks even when you perform all of the tasks yourself. Partitioning the tasks allows you to give your full attention to each individual task and thus perform better than you could if your tried to divide your attention between them while performing both at once. 
            
            
            Additionally, partitioning complexity allows you to distribute parts of it to people with other skills sets who do not have the interest, attention, or skills to do the whole thing, but who can do parts of it adequately, or even superbly.
            
           
            In some cases, we reduce the complexity that falls on each contributor by gathering a particular complex operation from the many and distributing it to one uniquely qualified or equipped person. For example, when we {separate content from formatting} we take formatting responsibility away from all writers and distribute it to a single designer who writes the formatting algorithm. In other cases, we do so by taking a complex operation currently being performed by a single person and distribute it out to many contributors. For example, linking is typically the sole responsibility of the writer writing the piece that contains the links. But the subject-domain approach to linking, which uses subject annotation rather than link markup, partitions and distributes the linking task three ways. The writer identifies significant subjects in their text. Other writers index or structure their content so that its subject matter can be identified clearly by algorithms, and the information architect or content engineer writes the linking algorithm (or may implement some other way of handling the deflection point in the content). In another example, the bottom up approach to taxonomy management partitions and distributes the task of vocabulary discovery out to the writers while distributing vocabulary management to a taxonomist. 
            

        section: Scale
        
            The question of whether it is worthwhile to transfer complexity from one part of a process to another often depends on scale. Transferring complexity from a human to an algorithm, for instance, creates a complex task of writing, testing, and maintaining the algorithm. But algorithms don't cost any more to write, test, and maintain based on the amount of content you process with them. You have to pay a human every time they do the task by hand. You only have to pay them once to write and algorithm to do it automatically. 
        
            All of the algorithms we have discussed so far can be carried out either by humans or by algorithms. Algorithms are much faster than humans, but algorithms have to be designed, written, and maintained, and they don't have the same capacity as humans to adapt on the fly when conditions warrant it. They also require much more precisely structured inputs than humans, meaning that they require more work from human writers, at least for original content creation. (They may save writers work in all sorts of other ways, as we have seen.)

            How do you determine when it is worth the investment to design structures and algorithms and have your writers write structured documents, rather than having them just execute all the algorithms themselves. Issues of consistency and quality are important considerations here. There are also some information designs that are very costly to develop without the aid of algorithms. However, one of the key factors in making this decision is scale. 
            
            The issue of scale applies to many systems and many activities. A home kitchen does not work like a restaurant kitchen. The restaurant kitchen has multiple work stations as divides the work up among multiple cooks. They prepare ingredients in advance so that when an order comes in, they can prepare it very quickly. All the equipment, planning, and preparation costs money, but it pays off when you have to process a lot of food orders during a busy dinner shift. By contrast, a cook in a home kitchen starts with basic ingredients and does all the steps themselves to prepare a single meal for themselves or a small group. 
            
            The restaurant could not possibly keep up with orders if it worked the way a home kitchen worked. That method of preparation does not scale up to the throughput of a restaurant. Equally, though, the approach of a restaurant kitchen would not work for a home kitchen. The overhead of all the stations, the planning, and the advanced preparation would not be economical for preparing a single small meal. The approaches of a large-scale operation do not scale down to a small-scale operation anymore than the approaches of a small-scale operation scale up for a large organization. 
            
            The same is true of writing. 

        section: Avoid tool filters 
        
            It is very easy to fall into the trap of viewing our processes through the filter of our tools. Every tool reflects the tool-designer's view of how some or all of the complexity of the content system should be partitioned and directed. And since the partitioning and directing of the complexity of a system is the definition of process, tools are encapsulations of process. When you buy a tool, you buy the process it encapsulates, and long practice with the tools can shape how you view process. After a while, it is hard to manage a process in any other terms. Thus all too often when we spec a new tool, we essentially end up specking our old tool with some particular improvement we thing will make our lives better. But many cases, the improvement we are seeking is not compatible with the current tool. (If it were, the vendor would probably have included it in their ongoing attempts to drive upgrade sales through new features.) If your old tool won't cut it, chances are you need a different process from the one incorporated in that tool, and you are going to need to break through your tool filter to envision a new process. This is why this book has focused on algorithms and structures, not tools and systems: to help you overcome the tool filter in your process design decisions.

            Over the years I have seen many requirements documents for proposed structured writing systems that essentially said that the proposed system must work exactly like Microsoft Word. This is not surprising when the people writing the requirements have used nothing but Word to create content for years. The tools you know shape how you work and what you think of as possible. As Henry Ford is supposed to have said of the Model T, "If I asked customers what they wanted, they would have said faster horses." Even when we are dissatisfied with our current tools, we tend to want the same basic tool only more so. Faster horses. This is why so many structured writing tool vendors literally advertise that their editor looks and feels "just like Microsoft Word". (Not to mention those vendors who create tools that modify Word itself.)

            But Microsoft Word is a tools that sits on the boundary between the Media and Document domains. Using Word itself, or something that looks like Word, is usually an attempt to move its use slightly more into the document domain, but as we have seen, the WYSIWYG authoring interface invites a slide back into the media domain by hiding the structured that is supposed to be created and showing only the formatting that is supposed to have been factored out in adopting the document domain. 

            It is little to be wondered then that the structured writing tools that have been popular in the market to date have been predominantly document domain tools, and have tended, like DocBook, to be very loosely constrained. (It is much easier to write an XML document in a WYSIWYG editor if the underlying structures are minimally constrained, since it lets you insert whatever bit of formatting you want anywhere you want. 

            Even with tools like DITA, which, while it is still fundamentally a document domain system, is more constrained, and capable of being constrained further, tend to be used in its generic out of the box form and with a Word-like WYSIWYG interface.  

            Thus even when a decision-making process is based on business requirements rather that specific tools, it is often tacitly driven by existing tool sets and ways of doing things, because those existing tools and processes shape our view of what the business requirements actually are. We don't ask for a way to get from DesMoins to Albuquerque, we ask for a faster horse that eats few oats.   
    
    section: Select domains

        Once you have a good idea of where the complexity lies in your content system and how you would like to partition and direct that complexity, it is time to decide which domains you want to work in. This is not as simple as picking one of the three and using it for everything. There are some pieces of content for which no meaningful subject domain markup makes sense -- content that has no repeatable pattern of either subject matter or rhetoric. For this you will need generic markup in the document or media domain. Some content will need to be laid out by hand with an artists eye for design. That can only happen in the media domain. In practice, your content system is likely to include a mix of subject domain, document domain, and media domain content, with some measure of the management domain thrown in where needed. In fact, major public languages like DITA and DocBook contain structures from all four domains. 
        
        Also, the places where complexity is hurting your content is not necessarily the same for all content types. The complexities that attend the maintenance of a reference work are likely to be different from those that attend the creation of a full color print add in five languages. Different types of content requires different partitioning and direction of complexity, and so require different structure writing structures. 
        
        Ideally, the choice of domains would be simple. Given the complexities you have identified for partitioning and redirection, choose the domain that accomplished the desired partitioning. But in practice it is more complicated than that because structures and the algorithms that process them are themselves sources of complexity. For example, it may seem like DITA addresses the complexities of your content reuse problem. But it also introduces complexities into the authoring process, in the form of complex markup and management domain intrusions. It also introduces significant content management complexity, both because it produces so many small artifacts, but also because writers need a way to find reusable content. This creates a retrieval problem, which creates terminology management complexity. Handing these new complexities requires new tools, typically a DITA-aware structured editor and a DITA-aware Component Content Management System, as well as new roles and extensive training. And even with those things in place, there is still a lot of conceptual and management complexity that writers have to deal with, and you still have issues with rhetorical conformance because there is no way to constrain rhetorical blocks in DITA. 
        
        All of that additional complexity, and the cost of the tools, may be worth it if you can realize big enough gains from reuse without falling into its quality traps. For many organizations, it is the additional savings from reduced translation costs that swing the needle to the positive side, rather than reuse benefits alone. But some organizations have that they simply don't realize the amount of reuse that would justify the expense and complexity of their DITA systems. 
        
        DITA provides every document/management domain reuse algorithm in the book, but at a high cost in additional complexity. You might find you are better off with simple document domain systems that provide a less comprehensive suite of reuse features but inject less complexity into your system. Or they might be better off with {subject domain} reuse tactics, which are less comprehensive than DITAs document/management domain tools provide, but actually remove complexity from the writer rather than adding it. 
        
        Another source of complexity in the choice of domains is the amount of development you will have to do in house in order to implement them. {Media domain} tools like {Frame Maker} and public {document domain} tools like {DocBook} and {DITA} come with ready-made structures and algorithms. To implement a subject-domain strategy that is specific to your own content, you will have to develop some structures and algorithms yourself. This obviously adds a level of complexity to your process. 
        
        I should stress that even with {media domain} and public {document domain} tools, you will almost certainly have to do some structure and algorithm development, probably on an ongoing basis. Developing and maintaining FrameMake style sheets is structure and algorithm development, and many organization end up using tools like {FrameScript} to transfer mundane formatting or management tasks from writers to algorithms. If you want your DITA or DocBook output to match your brand, you are going to have to rewrite the formatting algorithms to create the look you want. If you do any kind of DITA specialization or DocBook customization, you are going to be in full structure and algorithm development mode, requiring all the same skills that would be needed to develop your own {subject domain} structures and algorithms.
        
        As we have seen throughout, the {subject domain} provides the most comprehensive set of structures and algorithms for addressing a wide range of structured writing algorithms. In particular, it does the best job of partitioning and directing complexity away from writers, which potentially open your system up to a wider range of contributors. And with the {subject domain}, most algorithms work on the same set of structures, as opposed to the {document}(concept "document domain") and {management domains}, in which each algorithms requires new structures. Finally, while there are more of them, subject domain structures tend to be small with few permutations of structure, meaning that their algorithms have less complexity to handle. All this helps contain development complexity. 

        Nonetheless, an organization that is looking to address the complexities of creating a bottom-up information architecture, for instance, might identify the {subject domain} as the best way to build such and architecture, but conclude that, for them, writing and maintaining the necessary structures and algorithms was too much complexity to introduce, and might decide to go with a Wiki or a {Markdown}-based solution. This would throw more of the responsibility for linking and for repeatability onto writers, but that might be the right balance to strike in how complexity is partitioned and directed in that organization. (Scale can play a big role in this decision. At a large scale, managing linking and repeatability by hand becomes onerous, which developing and maintaining {subject domain} structures is amortized over a lager body of content.)  

        We can usefully map the various content management domains in terms of two properties, complexity and distribution. Complexity measure the size of a language and its level of abstraction. Distribution measures the number of different languages required to represent a set of content. 
        
        >>>(image domainmap.svg)
            
    section: Select tools
    
        Once you have decided how you want to distribute complexity in your content system, you choose the languages, systems, and tools that allow you to implement that partitioning most efficiently. This may be a recursive process, since tools introduce complexity of their own which must be appropriately partitioned and distributed to make sure the tool does not introduce unhandled complexity into the system. As you begin to map tools into the system, pause to consider how the complexity that tool introduces will be handled. This may require changing other procedures or introducing other tools. If the tools introduces too much downstream complexity, of the complexity it introduces it hard to handle, you might need to consider a different tool, or even a different strategy for distributing complexity across your content system. 
    

        
        Inconsistency, duplication, delay, error, and failure are escape valves for the complexity the no part of the organization is willing to accept. Traditionally, a style guide has become the dumping ground for complexity. Every new rule and requirement gets written into an ever growing style guide. That guidance quickly grows beyond the capacity of any writer to remember of follow in every word they write. Dumping complexity in a style guide captures it, but does not handle it. Inevitably, it ends up trickling down to the reader. 
        
        The growth in unhandled complexity will often be interpreted (especially by the advocates of the system you have just installed) as a change management problem or a training problem. But while change management and training are both necessary any time you change how you do things, the most likely cause of problems after the installation of a new system is design problems, specifically a system design that redirects complexity away from itself without consideration for how it is going to be handled. 
        
        Change management is another scapegoat for complexity that no one will own. It is very common to blame the failure of structured writing systems on unwillingness of writers to change. The cure, the backers of the system assure us, is more training and more generalization of the change. But what is usually going on in these cases is that the system design has dumped new and unmanageable complexity on whatever group is rebelling -- almost always the writers, either the full time writers or the occasional contributors. 
    
        
        
        Every time you move your content further towards ideas in a head and further from dots on a page, you add another layer of algorithms to your content system. This can seem daunting, when you can buy an off-the-shelf document domain system with its main algorithms already written. But such systems are highly complex, require a lot of management, and still require a fair amount of algorithm development, if only to get the output formatted the way you want. 
        
        But more to the point, such systems often do not do a good job of the total partitioning of complexity in your content system. In particular they often dump more complexity on your authors than they remove, and that added complexity can make more demands on the attention of authors than they care sustain, resulting in that complexity being dumped downstream again and ending up in the reader's lap. 
        
        The subject domain is probably more foreign to your current experience than the document domain, and the thought of developing and maintaining the information architecture and content engineering skill sets necessary to implement it may be daunting. But the subject domain is fundamentally simpler and more robust than complex document domain solutions, and handles the partitioning and distribution of complexity better than either simple or complex document domain solutions. As I have pointed out many times, in the document domain, a new set of structures are required for each algorithm you want to apply, whereas in the subject domain the same set of structures inform all algorithms. This can provide an enormous cost saving and makes you content much less prone to the gradual degradation of content quality and integrity than can plague document domain and subject domain systems. 


        This is not about dropping a new tool into place to replace an old one in the same process. It is about redefining your process to bring aspects of its complexity under control that have never been properly managed in the past.

        Developing custom structures and algorithms and maintaining them is, of course, part of the complexity that tools always add to your content system, and so such development much be taken into account when you work out how to partition and distribute content complexity in your organization. But you should realize and accept that in some cases, without some degree of custom development it will be impossible to fully partition and direct complexity to people with the right skills, knowledge, and resources, without dropping any of that complexity and letting it fall through to your customers. 
        




        Many tools are designed to attack one form of complexity or another  and are careless about where the complexity they create, or the existing complexity that they create, falls. Many DTP and structured writing tools, in particular, sought to solve publishing or content management problems by dumping complexity on writers, effectively shutting out many people in the organization out of the content system, or forcing them to set up separate rival content systems that they could manage. And since even the trained authors were often having more complexity dumped on them than their attention could manage, a lot of the complexity got dumped on the reader as well. 


        section: DITA
        
            DITA is such a dominant feature of the structured writing landscape at the moment that it is necessary to say something about it. 
            
            DITA, on the other hand is based on a rhetorical theory about what the building blocks of a document are and how they go together. That theory may be very foreign to the way you write and almost certainly a departure from the tools you have been using. 

            People rushing to DITA for its reuse capabilities often seem to be unaware that they are also buying into a rhetorical theory. Paligo essentially is providing similar reuse capability sans the rhetorical theory and the constraints that go with it.
            
            In evaluating DITA it is important to remember that you are getting two things in one box: a set management domain structures optimized for every type of document domain content reuse, and the generic implementation of a particular rhetorical theory. You can get various bundles of reuse capability from a number of other tools without the rhetorical theory in tow. Logically, therefore, the reason to choose DITA would be that your principle needs are met by the combination of its two components: its reuse capability and its rhetorical theory. You can get the rhetorical theory alone from information mapping and the reuse capability alone from several other tools. 
            
            DITA's specialization feature is not merely a technical feature, it is an extension of its rhetorical theory. It is not a tool that is necessarily suitable for other approaches to rhetorical modeling even if it is technically capable of achieving them. It is optimized for the block and map model of document construction and it a clumsy instrument for rhetorical modeling at the document level. Since it should be obvious to the reader by now that I believe strongly in the power of rhetorical modeling at the document level -- it is one of the central pillars of the Every Page is Page One information design approach -- I am not a fan of DITA. Everything I say about DITA in this book should be understood through the lens.       
        
    section: Count the costs and savings   

        Most of what is written about structured writing focuses on cost savings, with the biggest arguments in its favor being focused on saving from content reuse and translation. I have chosen instead to focus this book on the management of complexity. The focus on cost reduction, while it makes for an easy sell to those who must ultimately fund any structured writing project, tends to lead to a focus on one particular cost in the system, and the systems introduced to reduce that cost often introduce complexity into the system that eats up all of the anticipated savings while damaging the quality of the output. All to many project result in higher costs and reduced productivity and quality. A focus on cost reduction, too, carries with it an implicitly admission that you can't think of ways to enhance the value of what you do. 

        A focus on complexity, on the other hand, addresses both cost and quality at the same time. Unhandled complexity, or complexity handled by a person or process without the skills or resources to handle it properly not only reduce quality, it also costs money. You may not be able to produce a neat (and misleading) spreadsheet that equates content reused with dollars saved, but a focus on comprehensive management of complexity can yield cost saving in all kinds of places, while also enhancing value. 
        
        Here is a look at some of the potential costs and savings from various structured writing practices. 
        
        Quality, change management, linking, etc can all be great sources of efficiency without recourse to the complexity and quality problems that plague many reuse schemes. Repeatability may save you far more money than reuse. It is reuse of design. Besides, reuse without repeatability is a recipe for poor quality and content management breakdown. 


        The economics of this decision are clearly complex. You may decide that the cost of creating and maintaining the most appropriate algorithms and structures in not worth the cost or quality improvements they promise. But hopefully that decision can be made with a full appreciation of the benefits that those algorithms are capable of delivering. But whatever you decide, make sure you understand how complexity is being distributed in the systems that you implement, and very conscious of the ability of those you are distributing it to to handle it, and the effect it will have on their productivity and the reliability of their work.     
        
        Much of the cost of writing a piece of content lies in the information design -- in figuring out what needs to be said and how to say it. Creating a repeatable testable subject-domain design allows you to reuse that design. Reusing design can potentially have a much greater savings multiplier than reusing content, and without any of the potential quality traps of content reuse.          



