chapter:(#chapter.publishing) Publishing

    <<<(annotations.sam)

    index:: type, term
        concept, publishing algorithm

    All structured writing must eventually be published. Publishing structured content mean transforming it from the domain in which it was created ({subject domain}(concept), {document domain}(concept), or the abstract end of the {media domain}(concept)) to the most concrete end of the media domain spectrum: dots on paper or screen.
    
    In almost all structured writing tools, this process is done in multiple steps. Using multiple steps makes it easier to write and maintain code and to reuse code for multiple purposes. 
    
    In this chapter, I am going to describe the publishing process as consisting of four basic algorithms which I have mentioned in passing in earlier chapter: the synthesis, presentation, formatting, and encoding algorithms. These four stages are formalized in the {SPFE architecture}(tool), which I will talk about later, but I think they are a fair representation of what goes on in most publishing tool chains, even if those tool chains don't divide responsibilities exactly as I describe them here, or make such clear separation between them as I do here.
    
    section: The Rendering Algorithm
    
        There is actually a fifth algorithm in the publishing chain, which we can call the rendering algorithm. The rendering algorithm is the one responsible for actually placing the right dots on the right surface, be that paper, screen, or a printing plate. But this is a low-level device-specific algorithm and no one in the structured writing business is likely to be involved in writing rendering algorithms. The closest we ever get is the next step up, the encoding algorithm. 

        The rendering algorithm requires some form of input to tell it where to place the dots. In writing, this usually comes in the form of something called a {page description language}(tool). Like it sounds, this is a language for describing what goes where on a page, but in higher level terms that describing where each dot of ink or pixel of light is placed. A page description language deals in things like lines, circles, gradients, margins, and fonts. 
        
        >>>(image ../graphics/rendering.png)
        
        One example of a page description language is {PostScript}(language). Here is the PostScript code for drawing a circle:
        
        ```(PostScript)
            100 100 50 0 360 arc closepath
            stroke
        ```  
        
    section: The Encoding Algorithm
    
        Since most writers are not going to write directly in a page description language, the page descriptions for your publication are almost certainly going to be created by an algorithm. I call this the encoding algorithm.

        While it is possible that someone responsible for a highly specialized publishing tool chain may end up writing a specialized encoding algorithm, most encoding algorithms are going to be implemented by existing tools that translate formatting languages into page descriptions languages. 
        
        There are several formatting languages that are used in content processing. They are often called typesetting languages as well. {XSL-FO}(language) (XSL - Formatting Objects) is one of the more commonly used in structured writing projects. {TeX}(tool) is another. 
        
        >>>(image ../graphics/encoding.png)
        
        Here is an example of XSL-FO that we looked at in [#chapter.single-sourcing]:
        
        ```(XSL-FO)
            <fo:block space-after="4pt">
               <fo:wrapper font-size="14pt" font-weight="bold">
                 Hard Boiled Eggs
               </fo:wrapper>
            </fo:block>
        ```
        
        You process XSL-FO using an XSL-FO processor such as {Apache FOP}(tool). Thus the XSL-FO processor runs the encoding algorithm, producing a page description language such as {PostScript} or {PDF}(language) as an output. 
        
        Writers are not likely to write in XSL-FO directly, though it is not entirely impossible to do so. In fact some boilerplate content such as front matter for a book does sometimes get written and recorded directly in XSL-FO. (I did this myself on one project.) But when you are constructing a publishing tool chain, you will need to select and integrate the appropriate encoding tools as part of your process. 
        
        The job of the encoding algorithm is to take a high level description of a page or a set of pages, their content and their formatting, and turn it into a page description language that lays out each page precisely. For publication on paper, or any other fixed-sized media, this involves a process called {pagination}(concept): figuring out exactly what goes on each page, where each line breaks, and when lines should be bumped to the next page. 
        
        It is the pagination function, for instance, that figures out how to honor the keep-with-next formatting in an application like Word or FrameMaker. It also has to work out how to deal with complex figures such as tables: how to wrap text in each column, how to break a table across pages, and how to repeat the header rows when a table breaks to a new page. Finally, it has to figure out how to number each page and then fill in the right numbers for any references that include a particular page number. 
        
        This is all complex and exacting stuff and depending on your requirements you may have to pay some attention to make sure that you are using a formatting language the is capable of doing all this the way you want it done. 
        
        Also, you are going to have to think about just how automatic you want all of this to be. In a high-volume publication environment you want it to be fully automatic, but this could involve accepting some compromises. For example, it is not uncommon for writers and editors to make slight edits to the actual text of a document in order to make pagination work better. This is very easy to do when you are working in the {media domain} in an application like {Word}(tool "Microsoft Word") or {FrameMaker}(tool). If you end up with the last two words of a chapter at the top of a page all by itself, for instance, it is usually possible to find a way to edit the final paragraph to reduce the word count just enough to pull the end of the chapter back to the preceding page. This sort of thing gets much harder to do when you are writing in the {document domain} or the {subject domain}, particularly if you are {single sourcing}(algorithm) content to more than one publication or {reusing content}(algorithm "content reuse") in many places. An edit that fixes one pagination problem could cause another, and a major reason for writing in those domains it to take formatting concerns off the author's plate.
        
        For Web browsers and similar dynamic media viewers, such as E-Book readers or help systems, the whole pagination process takes place dynamically when the content is loaded into the view port, and it can be redone on the fly if the reader resizes their browser or rotates their tablet. This means the publisher has very little opportunity to tweak the pagination process. They can guide it by providing rules such as keep-together instructions through things like {CSS}(language), but they obviously cannot hand tweak the text to make it fit better each time the view port is resized. 
        
        The formatting language for these kinds of media is typically {Cascading Style Sheets}(tool) (CSS). 
        
    section: The Formatting Algorithm
    
        The job of the formatting algorithm it to generate the formatting language that drives the encoding and pagination process. The formatting algorithm produces the {media domain} representation of the content from content in the {document domain}. 
                
        In the case of HTML output, the formatting algorithm generates HTML (with connections to the relevant CSS, JavaScript, and other formatting resources). This is the end of the publishing process for the Web, since the browser will perform the encoding and rendering algorithms internally. 
        
        In the case of paper output, the formatting algorithm generates a formatting language such as {TeX} or {XSL-FO} which is then fed to the encoding algorithm as implemented by a TeX or XSL-FO processor. In some cases, organizations use word processing or desktop publishing applications to tweak the formatting of the output by having the formatting algorithm generate the input format of those applications (typically {RTF}(language) for Word and {MIF}(language) for FrameMaker). This allows them to exercise manual control over pagination, but with an obvious loss in process efficiency. In particular, any tweaks made in these applications are not routed back to the source content, so they will have to be done again by hand the next time the content is published. 

    section: The Presentation Algorithm

        The job of the presentation algorithm is to determine exactly how the content is going to be organized as a document. The presentation algorithm produces a pure document domain version of the content.   
        
        >>>(image ../graphics/presentation.png)
        
        The organization of content involves several things:
        
        |Ordering| At some level, content forms a simple sequence in which one piece of information follows another. Authors writing in the document domain typically order content as they write, but if they are writing in the subject domain, they can choose how they order subject domain information in the document domain.
        
        |Grouping| At a higher level, content is often organized into groups. This may be groups on a page or groups of pages. Grouping includes breaking content into sections or inserting subheads, inserting tables and graphics, and inserting information as labeled fields. Authors writing in the document domain typically create these groupings as they write, but if they are writing in the subject domain, you may have choices about how you group subject domain information in the document domain. 
        
        |Blocking| On a page, groups may be organized sequentially or laid out in some form of block pattern. Exactly how blocks are to be laid out on the displayed page is a media domain question, and something that may even be  done dynamically. In order to enable the media domain to do this, however, the document domain must clearly delineate the types of blocks in a document in a way that the formatting algorithm can interpret and act on reliably. 
       
        |Labeling| Any grouping of content requires labels to identify the groups. This includes things like titles and labels on data fields. Again, these are typically created by authors in the {document domain}, but are almost always factored out when authors write in the {subject domain} (most labels indicate the place of content in the subject domain, so inserting them is a necessary part of reversing the factoring out of labels that occurs when you move to the subject domain).
       
        |Relating| Ordering, grouping, blocking, and labeling cover organization on a two dimensional page or screen. But content can be organized in other dimensions by creating non-linear relationships between pieces of content. This includes hypertext links and cross references. 
        
    section: Differential presentation algorithms
        
        The organization of content is an area where the {document domain} cannot ignore the differences between different media. Although the fact that a relationship exists is a pure document domain issue, how that relationship is expressed, and even whether it is expressed or not, is affected by the media and its capabilities. Following links in online media is very cheap. Following references to other works in the paper world is expensive, so document design for paper tends to favor linear relationships where document design for the web favors {hypertext}(concept) relationships. This is an area, therefore, in which you should expect to implement {differential single sourcing}(algorithm) and use different presentation algorithms for different media.
        
        >>>(image ../graphics/differential.png)
        
    section: Presentation sub-algorithms
       
        The presentation algorithm may usefully broken down into several  sub-algorithms, each dealing with a different aspect of the presentation process. How you subdivide your publishing algorithm is something you need to decide based on your particular business needs, but the following are some operations that it may well pay to treat as separate algorithms.
               
        section: The linking algorithm
       
            How content is linked or cross-referenced is a key part of how it is organized in different media, and a key part of differential single sourcing. We will look at the {linking algorithm}(algorithm) in detail in [#chapter.linking].
       
        section: The navigation algorithm
       
            Part of the presentation of a document or document set is creating the table of contents, index, and other navigation aids. Creating these is part of the presentation process. Because these algorithms create new resources by extracting information from the rest of the content, it is often easier to run these algorithm in serial after the main presentation algorithm has run. This also makes it easier to change the way a TOC or index is generated without affecting your other algorithms.
       
        section: The public metadata algorithm
       
            Many formats today contain embedded metadata designed to be use by downstream processes to find or manage published content. One of the most prominent of these is {HTML microformats}(language) which is used to identify content to other services on the web, including search engines. This is a case of subject domain information being included in the output. Just as subject domain metadata allows algorithms to process content in intelligent ways as part of the publishing process, subject domain metadata embedded in the published content allows downstream algorithms (such as search engines) to use the published content in more intelligent ways.
            
            If content is written in document domain structures, public metadata is generally created by writers as annotations on document domain structures. But if content is created in the subject domain, the public metadata is usually based on the existing subject domain structures. In this case the public metadata algorithm may translate subject domain structures in the source to document domain structures with subject domain annotations in the output.
            
            This does not necessarily mean that the public metadata you produce is a direct copy of subject domain metadata you use internally. Internally, subject domain structures and metadata are generally based on your internal terminology and structures that meet your internal needs. Public terminology and categories may differ from the ones that are optimal for your internal use. But because this is subject domain metadata, and thus rooted in the real world, there should be a semantic equivalence between your private metadata and the public metadata (the public usually being more generic and less precise than the private). The job of the public metadata algorithm, therefore, is not merely to insert the metadata but sometimes to translate it to the appropriate public categories and terminology.  
            
        section: The document structure normalization algorithm
            In many cases, content written in the subject domain will also include many document domain structures. If those document domain structures match the structures in the document domain formats you are creating, all the presentation algorithm needs to do is to copy them to the document domain. In some cases, however, the document domain structures in the input content will not match those required in the output, in which case you will need to translate them to the desired output structures. 
            
           
    section: The Synthesis Algorithm
   
        The job of the synthesis algorithm is to determine exactly what content will be part of a content set. It passes a complete set of content on to the presentation algorithm to be turned into one or more document presentations.     
        
        Among other things, the synthesis domain resolves all management domain structures in the content (unless some are to be retained for downstream post-publication algorithms to work with). This means that is processes all inclusions and evaluates all conditions. The result is document domain or subject domain content with all of the management structures removed and replaced with the appropriate document or subject domain structures and content. 
        
        In the case of document domain content, processing the management domain structures yields a document domain structure which may then be a pass-through for the presentation algorithm (that is, the document domain markup may already express the desired presentation). 
        
        In the case of the subject domain content, processing management domain structures yields a definitive set of subject domain structures which can be passed to the presentation algorithm for processing to the document domain.
        
        
        >>>(image ../graphics/synthesis.png)
        
    section: Differential synthesis
    
        We noted above that you can use differential presentation to do {differential single sourcing} were two publications contain the same content but organized differently. If you want two publications in different media to have differences in their content, you can do this by doing differential synthesis and including different content in each publication. 

        >>>(image ../graphics/differential-synthesis.png)
        
        
    section: Synthesis sub-algorithms
        
        The synthesis algorithm can involve a number of sub-algorithms, depending on the kind of content you have and its original source.

       
        section: The inclusion algorithm

            If your content contains {management domain} include instructions, such as we identified in discussing {the reuse algorithm}(algorithm "content reuse") these must be resolved and the indicated content found and included in the synthesis.
            
            As we noted in the chapter on {the reuse algorithm} you can also include content based on subject domain structures, without any {management domain} include instructions. Such inclusions are purely algorithmic -- meaning that the author does not specify them in any way. It is the algorithm itself that determines if and when content will be included and where it will be included from. This too is the job of the inclusion algorithm.
            
        section: The filtering algorithm
        
            If your content contains {management domain} {conditional structures}(concept) (filtering) they must be resolved as part of the synthesis process. In most cases, you will be using the same set of management domain structures across your content set, so maintaining your filtering algorithm separately makes it easier to maintain.

            Again note that you may be filtering on {subject domain} structures as well (or instead of) on explicit {management domain} filtering instructions. Such filtering is, again, purely algorithmic, meaning that the author has no input into it. The filtering algorithm is then wholly responsible for what gets filtered in and out and why. 
                       
        section: Coordinating inclusion and filtering
        
            It is important to determine the order in which inclusion and filtering are performed. The options are to filter first, to include first, or to include and filter iteratively.
            
            Generally you want the filtering algorithm to run before other algorithms in the synthesis process so that other algorithms do not waste their time processing content that is going to be filtered out. However, if you run the filtering algorithm before you run the include algorithm, any filtering that needs to happen on the included content will not get executed.
            
            Doing inclusion before you filter addresses this problem, but creates a new one. If you include before you filter, you may end up including content based on instructions or subject domain structures that are going to be filtered out, which could then leave you with a set of included content that was not supposed to be there, but no easy way to identify that it does not belong. 
            
            The preferred option, therefore, is to run the two algorithms iteratively. Filter your initial source content. When you filter in an include instruction, immediately execute the include and run the filtering algorithm on the included content, processing any further include instructions as they are filtered in.  
            
            The {rules based approach to content processing}(concept "rules-base processing") that we have looked at in this book makes this kind of iterative processing relatively easy. You simply include both the filtering and inclusion rules in one program file and make sure that you submit any included content for processing by the same rules the moment it is encountered.
            
            ```(pseudo)
                match include
                    process content at href
                    continue
            ```


        section: The extraction algorithm
        
            In some cases you may wish to extract information from external sources to create content. This can include data created for other purposes, such as application code or data created and maintained as a canonical source of information, such as a database of features for different models of a car. We will look at the extraction and merge algorithms in [#chapter.extract].
            

        section: The cataloging algorithm
        
            The synthesis algorithm will produce a collection of content, potentially from multiple sources. This collection is then the input to the presentation algorithm. For the presentation algorithm to do its job, it needs to know all of the content it has to work with. In particular, the TOC and index algorithms and the linking algorithm need to know where all the content is, what it is called, and what it is about. They can get this information by reading the entire content set, but this can be slow, and perhaps confusing if the structure is not uniform. As an alternative, you can generate a catalog of all the content that the synthesis algorithm has generated which can then be use by these and potentially other sub-algorithms of the presentation algorithm to perform operations and queries across the content set.
        
        section: The resolve algorithm
            
            When we create authoring formats for content creation, we should do so with the principal goal in mind of making it as easy as possible for authors to create the content we require of them (see [#chapter.authoring]. This means communicating with them in terms they understand. This may include various forms of expression that need to be clarified based on context before they can be synthesized with the rest of the content. This is the job of the resolve algorithm. Its output is essentially a set of content in which all names and identifications are in fully explicit form suitable for processing by the rest of the processing chain.
            
            Content written in the subject domain is not always written in a fully realized form. When we create subject domain structures, we put as much emphasis as we can on ease of authoring and correctness of data collection. Both these aims are served by using labels and values that make intuitive sense to authors in their own domain. For example, a programmer writing about an API may mention and markup a method in that API using a simple annotation like this:
        
            ```(sam)
                To write a Hello World program, use the {hello}(method) method.
            ```
            
            In your wider documentation set, there may be many APIs. To relate this content correctly in the larger scope you will need to know which API it belongs to. In other words, you need this markup:
            
            ```(sam)
                To write a Hello World program, 
                use the {hello}(method (GreetingsAPI)) method.
            ```
            
            The information in the nested parentheses is a namespace identifier. A namespace specifies the scope in which a name is unique. For instance, in most families, we only use first names to refer to each other because our first names are unique in our families. But when we go to school or to work, we often find that there are other people with the same first name. We then add people's last names when referring to them. The last names serves as a namespace identifier. We may need to add more specific namespace identifiers as we deal with larger groups of people (such as their addresses). But we don't use the full namespace identifiers of our family members at home. We add them as needed when we deal with the wider world.  
            
            In this case, the method name `hello` might occur in more than one API. The namespace specifies that this case of the name refers to the instance in the `GreetingsAPI`. 
            
            Rather than forcing the author to add this additional namespace information when they write (the equivalent of calling everyone by their full name and address around the dinner table), we can have the synthesis algorithm add it based on what it knows about the source of the content. This simplifies the author's task, which means they are more likely to provide the markup we want. (It is also another example of factoring out invariants, since we know that all method names in this particular piece of content will belong to the same API.)
            
            The namespace of a person's name is implicit in where they live. It is not that people lack a last name or address when they are at home. It is that we don't use those those additional identifiers because they are cumbersome and the first name is sufficiently unique to establish who we are talking about. Similarly, content created within a domain has its namespaces implicit within the domain in which it is created. 
            
            When we pull content from multiple domains, however, as we might do for purposed of {content reuse} or simply because we have divided authoring of a large content set into multiple domains to make it easier to create and manage, we need to make the namespaces of all the local names we use explicit in the content as we pull it out of the local domain. This can be done by algorithm because the algorithm can know which domain each peices of content is coming from. 
            
    section: Deferred synthesis
        
        For static presentation, all synthesis happens before the material is presented. But if you are presenting content on the web, you can defer parts of the synthesis algorithm to the browser, which can synthesize and present content by making calls to web services or other back-end data source, or by making a request to code running on the server to synthesize and present part of the page. 
        
        # more?
    
    section: Combining algorithms
            
        As we have seen, structured writing algorithms are usually implemented as sets of rules that operate on structures as they encounter them in the flow of the content. Since each algorithm is implemented as a set of rules, it is possible to run two algorithms in parallel by adding the two sets of rules together to create a single combined set of rules that implements both algorithms at once. 
            
        Obviously, care must be taken to avoid clashes between the two sets of rules. If two set of rules act on the same structure, you have to do something to get the two rules that address that structure to work together. (Different tools may provide different ways of doing this.)
        
        In other cases, though, one algorithm needs to work with the output of a previous algorithm, in which case, you need to run them in serial. 
        
        In most cases, the major algorithms (synthesis, presentation, formatting, encoding, and rendering) need to be run in serial, since they transform an entire content set from one domain to another (or from one part of a domain to another). In many cases the sub-algorithms of these major algorithms can be run in parallel by combining their rule sets since they operate on different content structures.
        
    section: The consistency challenge
    
        The biggest issue for every algorithm in the publishing chain is consistency. Each step in the publishing chain transforms content from one part of the content spectrum to another, generally in the direction of the media domain. 
        
        The more consistent the input content is, the easier it is for the next algorithm in the chain to apply a simple and reliable process to produce consistent output, which in turn makes the next algorithm in the chain simpler and more reliable. 
        
        Building in consistency at source is therefore highly desirable for the entire publishing algorithm. This is an interesting problem because good content by it nature tends to be diverse. Content is the way that we convey the information that is by its nature less consistent and more prone to exceptions. It is the stuff that does not fit easily into rows and columns.
        
        One approach to this problem is to write all the content in a single {document domain} language such as DocBook. Since all the content is written in a single language it is theoretically completely consistent and therefore should be easy to process for the rest of the tool chain. 
        
        The problem with this is that any document domain language that is going to be useful for all the many kinds of documents and document designs that many different organizations may need is going to contain a lot of different structures, some of which will be very complex and most of which will have lots of optional parts. This means that there can be thousands of different permutations of DocBook structures. A single formatting algorithm that tried to cover all the possible permutations could be very large and complex and potentially very hard to maintain. 
        
        Another alternative it to have authors write in small simple subject domain structures that are specific to your business and your subject matter. You would then transform these to a document domain language using the presentation algorithm. This document domain language could still be DocBook, but now that you control the DocBook structures that are being created by the presentation algorithms, you don't have to deal with all the complexities and permutation that an author might create in DocBook, just the structures that you know your presentation algorithm creates.
        
        These subject domain documents would have few structures and few options and therefore few permutations. This would mean that the presentation algorithms for each could be simple, robust, and reliable, as well as easy to write and to maintain. You would also be able to do {differential single sourcing} by writing different presentation algorithms for each media or audience.
        
        The trade-off, of course, is that you have to create and maintain the various subject domain formats you would need and the presentation algorithms that go with them. Its a trade-off between several simple structures and algorithms and a few complex ones.
        
        This trade-off comes up time and time again in structured writing. We will see it again when we look at other algorithms.
    

