<?xml version="1.0" encoding="UTF-8"?>
<chapter>
<title>The extract and merge algorithms</title>
<annotations>
<p>Concepts</p>
<p><phrase><annotation type="concept">subject domain</annotation></phrase> <phrase><annotation type="concept">media domain</annotation></phrase> <phrase><annotation type="concept">document domain</annotation></phrase> <phrase><annotation type="concept">management domain</annotation></phrase> <phrase><annotation type="concept">subject-domain</annotation></phrase> <phrase><annotation type="concept">media-domain</annotation></phrase> <phrase><annotation type="concept">document-domain</annotation></phrase> <phrase><annotation type="concept">management-domain</annotation></phrase></p>
<p>Languages</p>
<p><phrase><annotation type="language">SAM</annotation></phrase> <phrase><annotation type="language">DITA</annotation></phrase> <phrase><annotation type="language">DocBook</annotation></phrase> <phrase><annotation type="language">Markdown</annotation></phrase></p>
<p>Algorithms</p>
<p><phrase><annotation type="algorithm">publishing algorithm</annotation></phrase> <phrase><annotation type="algorithm">publishing</annotation></phrase> <phrase><annotation type="algorithm">synthesis algorithm</annotation></phrase> <phrase><annotation type="algorithm">synthesis</annotation></phrase> <phrase><annotation type="algorithm">presentation algorithm</annotation></phrase> <phrase><annotation type="algorithm">presentation</annotation></phrase> <phrase><annotation type="algorithm">formatting algorithm</annotation></phrase> <phrase><annotation type="algorithm">formatting</annotation></phrase> <phrase><annotation type="algorithm">encoding algorithm</annotation></phrase> <phrase><annotation type="algorithm">encoding</annotation></phrase> <phrase><annotation type="algorithm">composition algorithm</annotation></phrase> <phrase><annotation type="algorithm">composition</annotation></phrase> <phrase><annotation type="algorithm" specifically="composition">composability</annotation></phrase> <phrase><annotation type="algorithm">conformance</annotation></phrase> <phrase><annotation type="algorithm">conformance algorithm</annotation></phrase> <phrase><annotation type="algorithm">separating content from formatting</annotation></phrase> <phrase><annotation type="algorithm">single sourcing</annotation></phrase> <phrase><annotation type="algorithm">single sourcing algorithm</annotation></phrase> <phrase><annotation type="algorithm">single source of truth</annotation></phrase> <phrase><annotation type="algorithm">single source of truth algorithm</annotation></phrase></p>
</annotations>
<p>A great deal of the content we produce, particularly technical and business content, is essentially a report in human language on the specific features of a product, process, or data set. If those features are described in any kind of formal data set, such as a database or software source code, we can use structured writing techniques to extract information from those sources to create and/or validate content.</p>
<section>
<title>Tapping external sources of content</title>
<p>We have talked throughout this book about moving content from the <phrase><annotation type="concept">media domain</annotation></phrase> to the <phrase><annotation type="concept">document domain</annotation></phrase> and from the <phrase><annotation type="concept">document domain</annotation></phrase> to the <phrase><annotation type="concept">subject domain</annotation></phrase>. We have seen the advantages that come from creating content in the <phrase><annotation type="concept">subject domain</annotation></phrase>, and we have looked at the processing algorithms that can use <phrase><annotation type="concept">subject domain</annotation></phrase> content to produce various kinds of publications in different media.</p>
<p>Subject domain content is simply content that is created and annotated in structures that are based on the subject matter rather than on the structure of documents or media. Subject domain structures tell you what the content is about, rather than how it should be published. You can therefore write algorithms that processes it based on what it is about rather than how it is presented.</p>
<p>Any data source that is contained and annotated in <phrase><annotation type="concept">subject domain</annotation></phrase> structures is therefor a source of subject domain content, whether it was intended to produce content from or not. This includes virtually all databases and quite a bit of software code. It also includes all authored content anywhere available (under an appropriate license) that contains any usable subject domain structures or annotations. All of this is potential material for generating content as part of your overall publishing process. As such, the extract and merge algorithm can work effectively with many other structured writing algorithms.</p>
<p>Perhaps most obviously, extract and merge works with the composition algorithm. Extract and merge provides new sources of content for the composition algorithm to work with.</p>
<p>As a source of subject domain content, the extract and merger algorithm also naturally <phrase><annotation type="algorithm">separates content from formatting</annotation></phrase> and contributes to the <phrase><annotation type="algorithm">differential single sourcing algorithm</annotation></phrase>.</p>
<p>By tapping existing information to build content, the extract and merge algorithms also works hand in hand with the <phrase><annotation type="algorithm">content reuse algorithm</annotation></phrase>. In fact, it is really the highest expression of reuse, since it not only reuses content in the content system, but information from the organization at large -- a process that further reduces duplication within the organization.</p>
<p>Because the extract and merge algorithm taps directly into external sources of information, it is also a great source of information for the <phrase><annotation type="algorithm">conformance algorithm</annotation></phrase>. At one level, it provides a canonical source of information to validate existing content against. At another level, it factors out part of the conformance problem from the authoring function.</p>
<p>If the authoring function is required to conform to information in these sources, the best way to do this is to generate content directly from these sources. Then responsibility for the correctness of the content is shifted to the people who maintain the source you are extracting form. Since they were already responsible for the correctness of the information, this does not create any additional work for them, which means that there is a significant net gain in efficiency for the organization.</p>
</section>
<section>
<title>Information created for other purposes</title>
<p>There is nothing new, of course, about generating content from database records. Database reporting is a highly important and sophisticated field in its own right and it would be entirely correct to characterize it as a type of structured writing. What sets it apart, largely, from other structured writing practices, is that the databases it reports on serves other business purposes besides being sources of content. An insurance company policy database, for instance, may be used to publish custom benefit booklets for plan participants as well as for processing claims. The design of the structures and data entry interfaces of these systems has tended to fall outside the realm (and the notice) of writers and authoring system designers.</p>
<p>This is a pity, because it has often resulted in organizations developing separate processes, tools, and repositories for content creation in which the information already contained in databases is researched, validated, recorded, and managed entirely independently on the content side of the house. Rather than treating code and databases as sources of content, writers treat them as research sources. They look information up in these sources and then go away and write content (in a separate repository) to relate the information from those sources.</p>
<p>The essence of the problem is that many content organizations choose to work in the <phrase><annotation type="concept">media domain</annotation></phrase> or the <phrase><annotation type="concept">document domain</annotation></phrase> and have neither the tools not the expertize to bridge the gap to all this material already available in the <phrase><annotation type="concept">subject domain</annotation></phrase>. But even when content organizations do extend their efforts into the <phrase><annotation type="concept">subject domain</annotation></phrase>, they are often blind to the fact that the <phrase><annotation type="concept">subject domain</annotation></phrase> content they are proposing to create already exists in the systems of another department.</p>
<p>Another way in which this is a pity is that when content is produced from these existing systems, for instance by a database reporting process, it exists in isolation from the rest of the content produced by the organization. Such content can often be quite sophisticated and beautifully formatted and published. But it is the product of an entire structured publishing chain that has to be separately developed and maintained.</p>
<p>In the field of software documentation we see the same pattern in regard to programming language API documentation. Much of the matter of an API reference guide is a description of each function, what information is required as input (its parameters or arguments), the information it produces as output, and the errors or exceptions that it can generate. All of this information already exists in the code that implements the function.</p>
<p>API documentation tools such as JavaDoc or Sphinx extract this information and turn it into human language content. They also combine this information with formal comments written into the code itself to create an completely human language description of the function. This is an application of subject-domain structured writing and the API documentation tools that do this implement an entire structured publishing system internally, producing final output, often in multiple formats.</p>
<p>And here we see all the same problems again:</p>
<ul>
<li>
<p>An entire publishing chain is maintained separately from the main content publishing chain.</p>
</li>
<li>
<p>The content produced from this publishing chain is isolated from all the other content produced by the organization.</p>
</li>
<li>
<p>Much of the same information is often created and maintained separately in a different repository and tool chain in the form programming guides and/or knowledge base articles.</p>
</li>
</ul>
<p>There are other cases of entirely separate publishing chains producing information that is isolated from the rest of an organization’s content. Technical support organizations, for instance, commonly create knowledge bases to answer commonly asked questions. The material in these knowledge bases is technical communication, plain an simple, yet it usually exists in isolation from the product documentation set, even to the extent that uses may not be able to search both the documentation and the knowledge base from the same search box. Most users, however, have no way of guessing whether the answer they are looking for is going to be in the docs or the knowledge base (or in the users forum, often yet another independent publishing system.)</p>
<p>There are a couple of ways to address these redundancies and the isolation that goes with them. One is to attempt to unify all content authoring and production in a single enterprise-wide system, often with a single set of content structures intended for use across all enterprise departments. However, this is a highly expensive and disruptive approach and tends to create interfaces and structures that are less usable and less specific to various business functions than the ones they replace. It also ignores the fact that many of the systems from which we wish to extract content exist for other purposes besides the content that is generated from them. Their subject-domain structures are specific and necessary to the database functions or software code generation they were built for.</p>
<p>Another approach is to leave the subject domain systems in place (and create more of them) and feed their output into a common <phrase><annotation type="concept">document-domain</annotation></phrase> publishing tool chain. It is a normal part of the <phrase><annotation type="algorithm">publishing algorithm</annotation></phrase> for <phrase><annotation type="concept">subject-domain</annotation></phrase> content to pass through the document domain on its way to <phrase><annotation type="concept">media-domain</annotation></phrase> publication. Subject domain content, by its nature, is not strongly tied to a particular document domain structure, so integrating many sources of subject domain content into a single publishing chain is not particularly onerous. (Specific management domain features of certain tool chains make things more complicated, but since the subject domain tends to factor out a lot of the management domain, this is not an insurmountable problem.)</p>
<p>Most enterprise-wide content systems are based on <phrase><annotation type="concept">document-domain</annotation></phrase> languages. (There is, after all, no way to create a single enterprise-wide subject-domain system, since an enterprise create content on many subjects.) In principle, a document-domain system should be capable of integrating content from domain-specific subject-domain systems.  Unfortunately, it is not common for either the subject domain systems or the enterprise content systems to be designed with this kind of integration in mind.</p>
<p>Because of this, we sometimes have to find ways to extract content from these sources and feed them into a unified publishing chain. This create the need for extraction and merge algorithms (which are part of the <phrase><annotation type="algorithm">synthesis algorithm</annotation></phrase> within the overall <phrase><annotation type="algorithm">publishing algorithm</annotation></phrase>).</p>
</section>
<section>
<title>The Extraction Algorithm</title>
<p>A common example of the extraction algorithm is found in API documentation tools such as JavaDoc. These tools parse application source code to pull out things like the names of functions, parameters, and return values, which it then uses to create the outline, at least, of reference documentation. Essentially, it generates a human language translation of what the computer language code is saying.</p>
<p>How the extraction algorithm works depends entirely on how the source data is structured, but it should usually create output in the subject domain that clearly labels the pieces of information it has culled from the source. For instance, a Java function definition is a piece of structured content in which the role and meaning of each element is known from the pattern and syntax of the Java languages (its grammar):</p>
<codeblock language="java">
boolean isValidMove(int theFromFile, int theFromRank, int theToFile, int theToRank) {
        // ...body
    }
</codeblock>
<p>This same informaiton can be extracted by a process that knows the grammar of Java to produce something that looks more like subject domain content:</p>
<codeblock language="sam">
java-function:
    name: isValidMove
    return-type: boolean
    parameters:: type, name
        int, theFromFile
        int, theFromRank
        int, theToFile
        int, theToRank
</codeblock>
<p>This is the same information, but in a different structure. In this structure, however, it is easily accessible by content processes and can then be processed through the rest of the publishing tool chain just like any other content.</p>
<p>The only problem here is that while this is useful content, there is not enough detail here to build an API reference with this information alone. A good reference entry also requires some explanation of the purpose of the function, a little more detail on its parameters, and possibly a code sample illustrating its use.</p>
</section>
<section>
<title>The merge algorithm</title>
<p>To address this, we can merge authored content covering these topics with the content we have extracted from the source.</p>
<p>In the case of API documentation tools, the authored content for merging is often written in the source code files. It is contained in code comments and is often written in small <phrase><annotation type="concept">subject domain</annotation></phrase> tagging languages that are specific to that tool. (Though as with all subject domain structures, any other tool can read them if it wants to.)</p>
<p>Here is an example of authored content combined with source code in JavaDoc<citation type="citation" value="https://en.wikipedia.org/wiki/Javadoc#Example"/>:</p>
<codeblock language="Java">
/**
 * Validates a chess move.
 *
 * Use {@link #doMove(int theFromFile, int theFromRank, int theToFile, int theToRank)} to move a piece.
 *
 * @param theFromFile file from which a piece is being moved
 * @param theFromRank rank from which a piece is being moved
 * @param theToFile   file to which a piece is being moved
 * @param theToRank   rank to which a piece is being moved
 * @return            true if the move is valid, otherwise false
 */
boolean isValidMove(int theFromFile, int theFromRank, int theToFile, int theToRank) {
    // ...body
}
</codeblock>
<p>In this example, everything between the opening <phrase><annotation type="code">/*</annotation></phrase> and the closing <phrase><annotation type="code">*/</annotation></phrase> is a comment (as far as Java itself is concerned), and the rest is a function definition in Java. However, JavaDoc sees the comment block as a block of structured text using a style of <phrase><annotation type="concept">markup</annotation></phrase> specific to JavaDoc.</p>
<p>The JavaDoc processor will extract information from the function definition itself (the extract algorithm) and then merge it with information from the authored structured content (the merge algorithm). In doing so, it has the chance to validate the authored content (the <phrase><annotation type="algorithm">conformance algorithm</annotation></phrase>), for instance by making sure that the names of parameters in the authored content match those in the function definition itself. This ability to validate authored content against extracted data is an important part of the <phrase><annotation type="algorithm">conformance algorithm</annotation></phrase>.</p>
<p>However, the merge algorithm does not require that the authored content be part of the same file as the data you will be extracting other information from. You can just as easily place the authored content in a separate file. All you need to be able to merge the too is an unambiguous key that you can find in the source data. You then enter that key as a field in the authored content where it can be used to match the authored content to the relevant extracted data.</p>
<p>One of the downsides of API documentation tools like JavaDoc is that they tend to be tightly coupled systems that produce media domain output such as formatted HTML directly, often providing little or no control over presentation or formatting. This is a problem because it means that your API reference content does not look like the rest of your content. And worse, it is not integrated with or linked to the rest of your content. This has obvious consequences like mentions of API routines in you programmer’s guide not being linked to the documentation of that routine in the API reference. It would be much better to generate subject domain content from the API documentation tool and then process it with the rest of your content. For many tools this is actually possible because many of them offer an XML output which may be either <phrase><annotation type="concept">subject domain</annotation></phrase> or <phrase><annotation type="concept">document domain</annotation></phrase>. Even if it is document domain, it may be regular enough that you can extract the subject domain structures reasonably easily.</p>
<!-- examples? -->
</section>
<section>
<title>The Diversity of Sources</title>
<p>The term <phrase><annotation type="algorithm">single sourcing</annotation></phrase> can mislead us. Single source can lead us to think that it means all source content is kept in a single place. Some vendors of content management systems would like to encourage this interpretation. But a better definition is that each piece of information comes from a single source. That is to say, that each piece of information comes from only one source (this is an aspect of the <phrase><annotation type="algorithm">single source of truth algorithm</annotation></phrase>.</p>
<p>This has nothing to do with keeping it all in the same place. Nor is keeping all content in the same place a particularly useful approach to ensuring that it is only stored once. Ensuring that content is only stored once, a process formally called “normalization” is actually about making sure that information, and the repository in which it is stored, meets an appropriate set of constraints.</p>
<p>The set of constraints that defines a piece of information as unique are not universal. The definition of what constitutes unique for different pieces of information is complex and specific to the subject matter at hand. Furthermore, the business processes and systems that ensure that these constraints are followed are not universal, but specific to each function and organization.</p>
<p>This is not to say that there are never trivial differences between the ways in which different bodies within an organization store and manage information that should not be rationalized. There are all kind of isolated and ad hoc information stores in most organizations that could potentially be much more efficient and (vitally) much more accessible, with a degree of rationalization and centralization. But it does not follow at all that absolute centralization into a single system or a single data model is appropriate, useful, or even possible.</p>
<p>The best way to ensure that information is stored once is to have it stored in a system with the right constraints and the right processes for the people who create and manage that information. This will mean that an integrated publishing system will draw from diverse sources of information and content. The ability to extract content from these sources and to merge it with other content for publication is therefore central to an effective strategy.</p>
</section>
</chapter>
